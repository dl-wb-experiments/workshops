{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</figure>\n",
    "<center> <h1>OpenVINO™ Deep Learning Workbench: Optimizing and Analysing Pre-Trained Neural Networks</h1> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pictures/preview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-26T09:56:27.382889Z",
     "iopub.status.busy": "2021-08-26T09:56:27.382564Z",
     "iopub.status.idle": "2021-08-26T09:56:27.386235Z",
     "shell.execute_reply": "2021-08-26T09:56:27.385628Z",
     "shell.execute_reply.started": "2021-08-26T09:56:27.382853Z"
    }
   },
   "source": [
    "## 1. [Introduction](#intro)\n",
    "\n",
    "## 2. [OpenVINO™ Overview](#OV-overview)\n",
    "\n",
    "## 3. [OpenVINO™ Deep Learning Workbench](#DL-WB-overview)\n",
    "\n",
    "## 4. [OpenVINO™ API](#OV-API)\n",
    "\n",
    "## 5. [Practice](#tasks)\n",
    "\n",
    "## 6. [Bonus: Deploy your first OpenVINO application as a telegram bot](#bot)\n",
    "\n",
    "## 7. [Bonus: Next steps with OpenVINO](#next-steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction  <a name=\"intro\"></a>\n",
    "\n",
    "### Workshop Contributors\n",
    "\n",
    "<div style=\"display: block; text-align: center;\">\n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img style=\"border-radius: 50%;\" src=\"pictures/Demidovskij.jpg\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Alexander Demidovskij</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://github.com/demid5111\" style=\"display: inline-flex;\"><img src=\"./pictures/github.svg\" width=\"22px\" style=\"margin-right: 5px\">@demid5111</a>\n",
    "      </figcaption>\n",
    "    </figure>\n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img style=\"border-radius: 50%;\" src=\"pictures/Tarkan.jpg\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Yaroslav Tarkan</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://github.com/yatarkan\" style=\"display: inline-flex;\"><img src=\"./pictures/github.svg\" width=\"22px\" style=\"margin-right: 5px\">@yatarkan</a>\n",
    "      </figcaption>\n",
    "    </figure>\n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img style=\"border-radius: 50%;\" src=\"pictures/Tugaryov.jpg\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Artyom Tugaryov</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://github.com/artyomtugaryov\" style=\"display: inline-flex;\"><img src=\"./pictures/github.svg\" width=\"22px\" style=\"margin-right: 5px\">@artyomtugaryov</a>\n",
    "      </figcaption>\n",
    "    </figure>\n",
    "    <br>\n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img style=\"border-radius: 50%;\" src=\"pictures/Kashchikhin.jpg\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Andrej Kashchikhin</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://github.com/akashchi\" style=\"display: inline-flex;\"><img src=\"./pictures/github.svg\" width=\"22px\" style=\"margin-right: 5px\">@akashchi</a>\n",
    "      </figcaption>\n",
    "    </figure>\n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img style=\"border-radius: 50%;\" src=\"pictures/Savina.jpg\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Tatiana Savina</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://github.com/tsavina\" style=\"display: inline-flex;\"><img src=\"./pictures/github.svg\" width=\"22px\" style=\"margin-right: 5px\">@tsavina</a>\n",
    "      </figcaption>\n",
    "    </figure>    \n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img style=\"border-radius: 50%;\" src=\"pictures/Salnikov.jpg\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Igor Salnikov</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://github.com/SalnikovIgor\" style=\"display: inline-flex;\"><img src=\"./pictures/github.svg\" width=\"22px\" style=\"margin-right: 5px\">@SalnikovIgor</a>\n",
    "      </figcaption>\n",
    "    </figure>\n",
    "</div>\n",
    "\n",
    "### In This Workshop\n",
    "\n",
    "Welcome to the Deep Learning workshop, where you will find out how to start working with pre-trained neural networks and write your own AI application. For that, we will be using OpenVINO™ framework and its graphical interface Deep Learning Workbench. \n",
    "\n",
    "During this workshop, you will:\n",
    "\n",
    "1. Learn the basics of neural model analysis and optimization:\n",
    "    - what a model is and how it works\n",
    "    - how to measure its performance and analyze the quality\n",
    "    - how to tune the model for enhanced performance\n",
    "2. Write your own AI application that detects faces and recognizes emotions in images and video\n",
    "\n",
    "### The Deep Learning Era\n",
    "\n",
    "Deep Learning recently has gained wide popularity due to significant breakthroughs in the artificial neural networks area. It has enabled the development of various revolutionary AI applications. With Deep Learning, the algorithm does not need to be taught about the essential features. It can discover features from data on its own using a neural network. Generally, Deep Learning algorithms use massive amounts of data and aim to emulate the human brain capacity to observe, learn, and make decisions, particularly for highly complex tasks. \n",
    "\n",
    "![](pictures/why-deep-learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"display: flex;\">\n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img src=\"pictures/nn-vs-humans-contrast.png\" style=\"height: 300px;max-width: initial;\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Classification of images with high contrast</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://arxiv.org/pdf/1706.06969.pdf\">Source</a>\n",
    "      </figcaption>\n",
    "    </figure>\n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img src=\"pictures/nn-vs-humans.png\" style=\"height: 300px;max-width: initial;\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Humans vs. Neural Models the LFW (Labeled Faces in the Wild) dataset</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://arxiv.org/pdf/1404.3840.pdf\">Source</a>\n",
    "      </figcaption>\n",
    "    </figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform a specific AI task, the model is trained on the known data. During training, the model makes predictions about what the data represents. Any error in the prediction is used to strengthen the artificial neuron connections until the acceptable accuracy level is achieved. The process of training usually done just once and requires massive amount of data and powerful computing systems. \n",
    "\n",
    "The process of using a pre-trained model to make predictions against previously unseen data is called inference. Inference is performed on different devices, happens several times, and consists in feeding the images to the model. In this workshop we will work with model inference; in other words, we will execute the model already pre-trained to perform useful work.\n",
    "\n",
    "![](pictures/deep-learning.png)\n",
    "\n",
    "Author: Mark Robins, Intel, [Link](https://www.intel.ru/content/www/ru/ru/artificial-intelligence/posts/deep-learning-training-and-inference.html)\n",
    "\n",
    "Let's take a look at the inference on the real-life example of the vehicle detection model. The model running alongside the traffic camera is continuously processing a video to identify the cars approaching the intersection. \n",
    "1. When a car enters at the red light, several photos of this car are taken by the camera and fed to the model, which finds a license plate on the image and sends it for further processing.\n",
    "2. At the server, the first inference is run to localize the license plate in the image. The second inference is run to read the characters on the license plate. \n",
    "3. After this, the license plate information is sent to the data center, where an application checks for potential traffic violations. \n",
    "\n",
    "| Device| Data transmission costs | Power consumption|Сomputing resources|\n",
    "|:-----: | :-----: | :-----: | :-----: |\n",
    "|Traffic Camera   | Low| Low | Low |\n",
    "| Gateway Server| Average| Average| Average |\n",
    "|Data Center | High| High| Powerful|\n",
    "\n",
    "\n",
    "![](pictures/system.png)\n",
    "\n",
    "Author: Mark Robins, Intel, [Link](https://www.intel.ru/content/www/ru/ru/artificial-intelligence/posts/deep-learning-training-and-inference.html)\n",
    "\n",
    "Therefore, given the specific task and the cost of data transmission, it is often beneficial to bring the model inference closer to the target where the data was first received. In this case, the pre-trained model requires some accelerating modifications and compression. So let's find out how the OpenVINO™ toolkit can help in achieving better performance of the Deep Learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. OpenVINO™ Toolkit <a name=\"OV-overview\"></a>\n",
    "\n",
    "1. [How OpenVINO advances AI technologies](#customers) \n",
    "2. [Introduction to the Openvino toolkit](#OV-intro) \n",
    "3. [OpenVINO capabilities](#OV-capabilities)\n",
    "4. [OpenVINO components](#OV-components)\n",
    "5. [OpenVINO Open Model Zoo](#OMZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 How OpenVINO advances AI technologies <a name=\"customers\"></a>\n",
    "\n",
    "The Open Visual Inference & Neural Network Optimization (OpenVINO™) toolkit is a comprehensive toolkit for optimizing pre-trained Deep Learning models of various use cases to achieve high performance and prepare them for deployment on Intel® platforms. Based on latest generations of artificial neural networks, including convolutional neural networks (CNNs), recurrent and attention-based networks, the toolkit extends computer vision and non-vision workloads across Intel® hardware, maximizing performance. It accelerates applications with high-performance, AI and deep learning inference deployed from edge to cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/openvino-success-stories.png)\n",
    "![](pictures/sales-collage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Introduction to the OpenVINO™ toolkit <a name=\"OV-intro\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/about_vino.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 OpenVINO™ Capabilities <a name=\"OV-capabilities\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/openvino_toolkit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 OpenVINO™ Toolkit Components <a name=\"OV-components\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/additional_tools.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 OpenVINO™ Open Model Zoo <a name=\"OMZ\"></a>\n",
    "\n",
    "A model is a network that has been trained over a set of data using a certain framework. Since Deep learning technologies are used in various industrial\n",
    "applications, it is crucial to have an effective solution for each specific use case. OpenVINO Open Model Zoo provides a range of public and Intel pre-trained models to resolve a variety of different tasks, such as classification, object detection, segmentation and many others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/models.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deep Learning Workbench: OpenVINO™ Quickstart <a name=\"DL-WB-overview\"></a>\n",
    "\n",
    "1. [DL Workbench Capabilities](#DL-WB-capabilities)\n",
    "2. [DL Workbench Workflow](#DL-WB-workflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning Workbench (DL Workbench) is the official OpenVINO™ graphical interface designed to make the production of pre-trained deep learning models significantly easier.\n",
    "With DL Workbench you can start working with your deep learning model right from your browser: import a model, analyze its performance and accuracy, visualize the outputs, optimize and prepare the model for deployment in a matter of minutes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/openvino_toolkit-dl-wb-highlighted.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 DL Workbench Capabilities <a name=\"DL-WB-capabilities\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pictures/DL-WB-flow.png\" width=\"1400\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 DL Workbench Workflow <a name=\"DL-WB-workflow\"></a>\n",
    "\n",
    "1. [Open DL Workbench](#open-wb)\n",
    "2. [Import the Model](#import-model)\n",
    "3. [Import the Dataset](#import-dataset) \n",
    "4. [Benchmark the Model](#model-inference)\n",
    "5. [Analyze the Model](#analyze-model) \n",
    "6. [Optimize the Model](#optimize-model)\n",
    "7. [Profile the Model](#profile-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display button for opening DL Workbench\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"<button class=\"wb-button\" onclick=\"window.open(location.origin, '_blank');\">Open DL Workbench</button><style>.wb-button { display: flex; width: fit-content; margin: 20px auto; align-items: center; height: 50px; font-size: 18px; font-weight: 400; font-family: inherit; line-height: 20px; background-color: #003C71; border: 1px solid #003C71; color: #ffffff; cursor: pointer; border-radius: 4px; padding: 0 30px; box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.15);}</style>\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open DL Workbench <a name=\"open-wb\"></a>\n",
    "\n",
    "To start working with DL Workbench, click **Create Project** button to open the Create Project page.\n",
    "\n",
    "![](pictures/start_page_dl_wb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Model <a name=\"import-model\"></a>\n",
    "\n",
    "Our first step is to import the model. Select and import **face-detection-adas-0001** model from the Open Model Zoo. \n",
    "\n",
    "![](pictures/create_project_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with OpenVINO tools, you need to obtain a model in Intermediate Representation (IR) format. \n",
    "IR is the OpenVINO format of pre-trained model representation with two files:\n",
    "\n",
    "- XML file describing the network topology\n",
    "- BIN file containing weights and biases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/face_detection_model_import.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset <a name=\"import-dataset\"></a>\n",
    "\n",
    "Next, you will need to obtain the data to work with the model. The data can be in different formats, depending on the task for which the model has been trained. Learn more about these formats in the [documentation](https://docs.openvinotoolkit.org/latest/workbench_docs_Workbench_DG_Dataset_Types.html). \n",
    "\n",
    "In our case, we will take a set of images and use them as the validation dataset:\n",
    "\n",
    "1. Use the following **link to download the dataset**: [Download Dataset](https://github.com/dl-wb-experiments/face-hiding-workshop/files/7081607/dataset.zip).\n",
    "2. Unarchive it.\n",
    "3. Go to DL Workbench."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/create_project_dataset_import.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drag and drop the images from the archive to create the dataset, and click **Import**.\n",
    "\n",
    "![](pictures/dataset_wb_images_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark the Model <a name=\"model-inference\"></a>\n",
    "\n",
    "Now that we have imported our model, we want to check how fast it works. For that, let's create our first project. Select the model and the dataset by clicking on them. You can also choose a hardware accelerator on which the model will be executed. We will analyze how our model works on CPU since we have this device available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/create_project_completed.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the Model <a name=\"analyze-model\"></a>\n",
    "\n",
    "When the inference stage is finished, we can see the result of running our model on the CPU. Latency is the time required to process one image. The lower the value, the better. Throughput is the number of images (frames) processed per second. Higher throughput value means better performance. Now let's check how the model works and try to make it even faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/inference_results_face_detection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test how the model works, go to **Perform** and open **Visualize Output** tab. Select an image from the previously downloaded dataset, upload this image and visualize the model predictions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/visualize_face_detection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize the Model <a name=\"optimize-model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the recommended ways to accelerate your model performance is to use 8-bit integer (INT8) calibration.\n",
    "Calibration is the process of lowering the model precision from floating-point (for example, FP32 or FP16) to integer precision (INT8).\n",
    "A model in the INT8 precision takes up less memory and has higher throughput capacity.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantize the Model to Low Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/quantization.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post-Trainig Optimization Toolkit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In OpenVINO toolkit, INT8 calibration is performed by Post-Training Optimization Tool (POT). POT is used to calibrate a model and then execute it in the INT8 precision.\n",
    "To optimize the model, open **Optimize Performance** tab and select the INT8 method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/optimize_face_detection.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/optimization_settings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze the Improvements\n",
    "\n",
    "Go to the `face-detection-adas-0001` model page to check the performance boost after INT8 optimization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/model_page_compare.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the throughput. Remember that higher throughput value means better performance. Then open optimized model project.\n",
    "\n",
    "![](pictures/comparison_models.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"./pictures/INT8_meme.png\" width=\"500\">\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profile the Model <a name=\"profile-model\"></a>\n",
    "\n",
    "We can see that the model has become faster after calibration. \n",
    "You can further accelerate your model by configuring the optimal parameters specific to each accelerator: streams and batches.\n",
    "In simple terms, streams are the number of cores, and batches are the number of images fed to the model. \n",
    "\n",
    "<center><img src=\"pictures/batch_stream_diagram.png\" width=\"1050\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We suggest trying to infer the model with the following parameters: Streams - 4, Batches - 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pictures/inference_config.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/stream_batch_result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After ensuring that the selected set of batches and streams has accelerated the model, let's use this information to create our working application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s briefly recap what you have learned at this stage:\n",
    "\n",
    "1. What a model is and how it works\n",
    "2. How to measure its performance\n",
    "3. How to accelerate the model using INT8 calibration\n",
    "4. How sets of streams and batches affect the model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Learn OpenVINO™ API <a name=\"OV-API\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's learn how to infer a model of object detection use case with OpenVINO™ Python interface and build our application. Object Detection in Computer Vision is the task of finding objects of a certain class and highlighting them with bounding boxes.\n",
    "\n",
    "We will go through the following steps:\n",
    "\n",
    "1. [Import required modules](#Import) \n",
    "2. [Configure inference: path to a model and other data](#Configure)\n",
    "3. [Initialize the OpenVINO™ runtime](#Initialize)\n",
    "4. [Read the model](#Read)\n",
    "5. [Configure input data pre processing](#PreProcessing)\n",
    "6. [Make the model executable](#Make)\n",
    "7. [Compile the Model for the Device](#Compile)\n",
    "8. [Prepare an image for model inference](#Prepare)\n",
    "9. [Infer the model](#Infer)\n",
    "10. [Show predictions](#Show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Import Required Modules <a name=\"Import\"></a>\n",
    "\n",
    "Import the Python* modules that you will use in the sample code:\n",
    "- [OpenVINO](https://docs.openvinotoolkit.org/latest/openvino_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html) is an OpenVINO™ Python API module used for inference.\n",
    "- [cv2](https://docs.opencv.org/trunk/) is an OpenCV module used to work with images.\n",
    "- [NumPy](http://www.numpy.org/) is an array manipulation module used to process images as arrays.\n",
    "- [IPython](https://ipython.readthedocs.io/en/stable/index.html) is an IPython API used for showing images and videos in the notebook\n",
    "- [typing](https://docs.python.org/3/library/typing.html) is a module provides runtime support for type hints\n",
    "\n",
    "Run the cell below to import the modules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openvino.runtime import Core, Layout\n",
    "from openvino.preprocess import PrePostProcessor\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from IPython.display import HTML, Image, display\n",
    "\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Configure Inference <a name=\"Configure\"></a>\n",
    "\n",
    "Once you have the OpenVINO™ IR of your model, you can start experimenting with it by inferring it and inspecting its output. \n",
    "\n",
    "> **NOTE**: Copy the paths to the `.xml` and `.bin` files from the DL Workbench UI and paste them below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/model_path.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required parameters\n",
    "\n",
    "Parameter| Explanation\n",
    "---|---\n",
    "**model_xml**| Path to the `.xml` file of OpenVINO™ IR of your model\n",
    "**model_bin**| Path to the `.bin` file of OpenVINO™ IR of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model IR files\n",
    "face_detection_model_xml = './data/face-detection-adas-0001.xml'\n",
    "face_detection_model_bin = './data/face-detection-adas-0001.bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional Parameters\n",
    "\n",
    "Experiment with optional parameters after you go through the full workflow of the tutorial.\n",
    "\n",
    "Parameter| Explanation\n",
    "---|---\n",
    "**input_image_path**| Path to an input image. Use the `data/all.jpg` image placed in the directory of the notebook or, if you have imported a dataset in the DL Workbench, copy the path to an image in the dataset.\n",
    "**device**| Specify the [target device](https://docs.openvinotoolkit.org/latest/workbench_docs_Workbench_DG_Select_Environment.html) to infer on: CPU, GPU, or MYRIAD. Note that the device must be present. For this tutorial, use `CPU` which is known to be present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration parameters settings:\n",
      "    model_xml=./data/face-detection-adas-0001.xml,\n",
      "    model_bin=./data/face-detection-adas-0001.bin,\n",
      "    input_image_path=data/all.jpg,\n",
      "    device=CPU\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Input image file\n",
    "input_image_path = 'data/all.jpg'\n",
    "\n",
    "# Device to use\n",
    "device = 'CPU'\n",
    "\n",
    "print(\n",
    "f'''Configuration parameters settings:\n",
    "    model_xml={face_detection_model_xml},\n",
    "    model_bin={face_detection_model_bin},\n",
    "    input_image_path={input_image_path},\n",
    "    device={device}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Initialize the OpenVINO™ Runtime <a name=\"Initialize\"></a>\n",
    "\n",
    "Once you define the parameters, let's initiate the `Core` object that accesses OpenVINO™ runtime capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Core' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create an OpenVINO instance\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m core \u001b[38;5;241m=\u001b[39m \u001b[43mCore\u001b[49m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Core' is not defined"
     ]
    }
   ],
   "source": [
    "# Create an OpenVINO instance\n",
    "core = Core()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Read the Model <a name=\"Read\"></a>\n",
    "\n",
    "Put the OpenVINO IR of your model in the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the model from OpenVINO IR files\n",
    "face_detection_model = core.read_model(model=face_detection_model_xml, weights=face_detection_model_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMOVE (will be set in DL Workbench during import the model)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection_model.input().node.set_layout(Layout('NCHW'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Make the Model Executable <a name=\"Make\"></a>\n",
    "\n",
    "Reading a model is not enough to start a model inference. The model must be loaded to a particular abstraction representing a particular accelerator. In OpenVINO™, this abstraction is called *plugin*. A model compiled for a device becomes will be inferred in one of the next steps. \n",
    "\n",
    "After loading, we keep necessary model information such as inputs and outputs: `inputs` and `outputs`. Let's remember the input dimensions of your model:\n",
    "- `n` - input batch size\n",
    "- `c` - number of input channels. Often, it is `1` or `3`, which means that the model expects either a grayscale or a color image.\n",
    "- `h` - input image height\n",
    "- `w` - input image width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store input of the model\n",
    "face_detector_input = face_detection_model.input()\n",
    "\n",
    "# Store name of the model input\n",
    "face_detector_input_name = face_detector_input.any_name\n",
    "\n",
    "# Read the input dimensions: n=batch size, c=number of channels, h=height, w=width\n",
    "face_detector_input_shape = face_detector_input.shape\n",
    "\n",
    "n, c, face_detector_input_height, face_detector_input_width = face_detector_input_shape\n",
    "\n",
    "print(f'Face Detection model input dimensions: n={n}, c={c}, h={face_detector_input_height}, w={face_detector_input_width}')\n",
    "\n",
    "\n",
    "face_detection_output = face_detection_model.output()\n",
    "face_detection_output_shape = face_detection_output.shape\n",
    "\n",
    "print(f'Face Detection model output dimensions {list(face_detection_output_shape)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pictures/face_detection.png\" width=\"1050\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Set pre-processing step for the model <a name=\"PreProcessing\"></a>\n",
    "For proper data processing, it is important to understand what is the layout (order of measurements) of the input data and what data layout is expected by the model.\n",
    "In our case we will have input data in **NHWC** and the face detector expects **NCHW** data layout.\n",
    "\n",
    "Let's check what data layout is expected by the face detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detector_input_layout = face_detection_model.inputs[0].node.layout\n",
    "print(f'The face detector expects {face_detector_input_layout} of the input data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the model expects another input data layout we must to say which layout of the input tensor will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PreProcessor to define pre processing manipulations that OpenVINO should do before Inference \n",
    "pre_processor = PrePostProcessor(face_detection_model)\n",
    "# We will use OpenCV to manipulate with images\n",
    "# OpenCV reads images in NHWC layout\n",
    "input_image_layout = Layout('NHWC')\n",
    "# Set input image layout to pre processor \n",
    "pre_processor.input(0).tensor().set_layout(input_image_layout)\n",
    "# Save changes\n",
    "face_detection_model = pre_processor.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Compile the Model for the Device <a name=\"Compile\"></a>\n",
    "\n",
    "This step is necessary to infer the model. You can learn more in the [documentation](https://docs.openvinotoolkit.org/latest/ie_python_api/classie__api_1_1IECore.html#ac9a2e043d14ccfa9c6bbf626cfd69fcc). **WRONG link**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "face_detector = core.compile_model(model=face_detection_model, device_name=device)\n",
    "print(f'Loaded the model into the {device} device.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 Prepare an Image for Model Inference <a name=\"Prepare\"></a>\n",
    "\n",
    "Now let's read and prepare the input image by resizing and re-arranging its dimensions according to the input dimensions of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the function to load the input image\n",
    "def load_input_image(input_path):   \n",
    "    # Use OpenCV to load the input image\n",
    "    return cv2.imread(input_path)\n",
    "\n",
    "# Define the function to pre-process (resize, transpose) the input image\n",
    "def pre_process_input_image(image: np.ndarray, target_height: int, target_width: int) -> np.ndarray:\n",
    "    # Resize the image dimensions from image to model input w x h\n",
    "    resized_image = cv2.resize(image, (target_width, target_height))\n",
    "    \n",
    "    n = 1 # Batch is always 1 in our case\n",
    "    c = 3 # Channels is always 3 in our case\n",
    "    \n",
    "    # Reshape to input dimensions\n",
    "    reshaped_image = resized_image.reshape((n, target_height, target_width, c))\n",
    "    return reshaped_image\n",
    "\n",
    "# Define the function to show the image\n",
    "def show_image(image: np.ndarray):\n",
    "    # Encode ndarray\n",
    "    _, data = cv2.imencode('.jpg', image) \n",
    "    \n",
    "    #  Create IPython.display.Image instance to display the image in the notebook\n",
    "    image = Image(data=data)\n",
    "\n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_image = load_input_image(input_image_path)\n",
    "show_image(original_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the shape of the original image to scale inference results in the future\n",
    "original_image_height, original_image_width, *_ = original_image.shape\n",
    "\n",
    "# Prepare the image\n",
    "input_frame = pre_process_input_image(original_image, target_height=face_detector_input_height, target_width=face_detector_input_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10 Infer the Model <a name=\"Infer\"></a>\n",
    "\n",
    "Now let's proceed to the inference by feeding the prepared image to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection_inference_results = face_detector.infer_new_request(\n",
    "    inputs={\n",
    "        face_detector_input_name: input_frame\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we print the inference results: a dictionary, where the keys are the names of the outputs, and the values are the inference results for each output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(face_detection_inference_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the needed inference results for a single output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection_inference_results = face_detection_inference_results[face_detector.output()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.11 Show Predictions <a name=\"Show\"></a>\n",
    "\n",
    "The next step is to parse the inference results and draw boxes over the objects detected in the image.\n",
    "\n",
    "The result of model inference (`face_detection_inference_results`) is an array of predictions. Each prediction `object` has the following structure:\n",
    "\n",
    "- `object[0]`: information about whether the detected object is a background\n",
    "- `object[1]`: predicted class ID\n",
    "- `object[2]`: сonfidence level that currently detected object is an instance of the predicted class\n",
    "- `object[3]`: lower x coordinate of the detected object \n",
    "- `object[4]`: lower y coordinate of the detected object\n",
    "- `object[5]`: upper x coordinate of the detected object\n",
    "- `object[6]`: upper y coordinate of the detected object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function for converting the initial results of the face detector into a more convenient structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_face_detection_results(inference_results: np.ndarray, \n",
    "                                 original_image_height: int,\n",
    "                                 original_image_width: int, \n",
    "                                 prob_threshold: float = 0.8) -> list:\n",
    "    # Prepare a list to save the detected faces \n",
    "    detected_faces = []\n",
    "    \n",
    "    # Iterate through all the detected faces\n",
    "    for inference_result in inference_results[0][0]:\n",
    "        \n",
    "        # Get the probability of the detected face and convert it to percent\n",
    "        probability = inference_result[2]\n",
    "\n",
    "        # If confidence is more than the specified threshold, draw and label the box \n",
    "        if probability < prob_threshold:\n",
    "            continue\n",
    "\n",
    "        # Get coordinates of the box containing the detected object\n",
    "        xmin = int(inference_result[3] * original_image_width)\n",
    "        ymin = int(inference_result[4] * original_image_height)\n",
    "        xmax = int(inference_result[5] * original_image_width)\n",
    "        ymax = int(inference_result[6] * original_image_height)\n",
    "        confidence = round(probability * 100, 1)\n",
    "        \n",
    "        detected_face = (xmin, ymin, xmax, ymax, confidence)\n",
    "        detected_faces.append(detected_face)\n",
    "            \n",
    "    return detected_faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the detected faces from the inference results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parse faces  \n",
    "detected_faces = parse_face_detection_results(face_detection_inference_results, original_image_height, original_image_width)\n",
    "detected_faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function to process inference results of the face detector: draw boxes in the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process inference results of the face detector \n",
    "def draw_boxes_of_detected_faces(detected_faces: list, original_image: np.ndarray) -> np.ndarray:\n",
    "    processed_image = original_image.copy()\n",
    "    \n",
    "    # Get output results\n",
    "    color = (12.5, 255, 255)\n",
    "    \n",
    "    # Loop through all possible results\n",
    "    for detected_face in detected_faces:\n",
    "        xmin, ymin, xmax, ymax, confidence = detected_face\n",
    "\n",
    "        # Draw the box and label for the detected object\n",
    "        cv2.rectangle(processed_image, (xmin, ymin), (xmax, ymax), color, 4)\n",
    "        cv2.putText(processed_image, \n",
    "                    f'{confidence} %', (xmin, ymin - 7), \n",
    "                    cv2.FONT_HERSHEY_COMPLEX, 1, color, 2)\n",
    "            \n",
    "    return processed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw boxes of the faces detected in the image\n",
    "processed_image = draw_boxes_of_detected_faces(detected_faces, original_image)\n",
    "\n",
    "show_image(processed_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO BE DONE: Upload the mask detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get mask detection model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model IR files\n",
    "mask_detection_model_xml = '/Users/atugarev/Developer/resources/models/mask_detector/mask_detector.xml'\n",
    "mask_detection_model_bin = '/Users/atugarev/Developer/resources/models/mask_detector/mask_detector.bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read the model \n",
    "Call core.read_model to read the OpenVINO IR model. The method has two arguments:\n",
    "    \n",
    "    1. model - path to the xml model file\n",
    "    2. weights - path to the bin model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_detection_model = core.read_model(model=mask_detection_model_xml, weights=mask_detection_model_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get the input name and shape of the model \n",
    "Get the input name and shape of the mask detector. For more information about the model, refer the [documentation](https://docs.openvinotoolkit.org/latest/omz_models_model_emotions_recognition_retail_0003.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask_detector_input = mask_detection_model.input()\n",
    "mask_detector_input_name = mask_detector_input.any_name\n",
    "\n",
    "mask_detector_input_shape = mask_detector_input.shape\n",
    "mask_detector_input_layout = mask_detector_input.node.layout\n",
    "\n",
    "n, mask_detector_input_height, mask_detector_input_width, c = mask_detector_input_shape\n",
    "\n",
    "mask_detector_output = mask_detection_model.output()\n",
    "mask_detector_output_shape = mask_detector_output.shape\n",
    "\n",
    "print(f'Input layout of the mask detector model is {str(mask_detector_input_layout)}')\n",
    "print(f'Input shape of the mask detector model is n={n}, c={c}, h={mask_detector_input_height}, w={mask_detector_input_width}')\n",
    "\n",
    "print(f'Output shape of the mask detector model is {mask_detector_output_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pictures/emotion_recognition.png\" width=\"1050\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Compile the model for the device\n",
    "\n",
    "Use the instance of `Core`.\n",
    "The class `Core` has a special function called `compile_model`, which compiles a model for a device.\n",
    "This method prepares the model for nference on the device \n",
    "and returns an instance of the network prepared for an inference (execution). \n",
    "This function has many parameters, but in this case, you need to know only about two of them:\n",
    "* `model` - instance of `Model`\n",
    "* `device_name` - string, contains a device name to infer a model on CPU, GPU and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask_detector = core.compile_model(model=mask_detection_model, device_name=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Crop the detected face from the full image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the first face from the example image \n",
    "first_face = detected_faces[0]\n",
    "\n",
    "# Unzip the face coordinates\n",
    "xmin, ymin, xmax, ymax, _ = first_face \n",
    "\n",
    "# Get the face from the original image \n",
    "example_image = original_image[ymin:ymax, xmin:xmax]\n",
    "print('Original image:')\n",
    "show_image(example_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Prepare the function for inference\n",
    "\n",
    "For better understanding of the model inference results, refer to the [documentation](https://github.com/chandrikadeb7/Face-Mask-Detection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_detector_inference(face_frame: np.ndarray) -> np.ndarray:\n",
    "    # 1. Prepare the image\n",
    "    prepared_frame = pre_process_input_image(face_frame, \n",
    "                                             target_height=mask_detector_input_height, \n",
    "                                             target_width=mask_detector_input_width)\n",
    "    # 2. Infer the model\n",
    "    inference_results = mask_detector.infer_new_request({\n",
    "        mask_detector_input_name: prepared_frame\n",
    "    })\n",
    "    # 3. Return predictions\n",
    "    return inference_results[mask_detector.output()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_detection_inference_results = mask_detector_inference(example_image)\n",
    "mask_detection_inference_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function to infer model and a simple processing of the inference results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_masked(face_frame: np.ndarray) -> bool:\n",
    "    mask_detection_results = mask_detector_inference(face_frame)\n",
    "    # The first value - probability of the masked face, the second - unmasked\n",
    "    return mask_detection_results[0][0] > mask_detection_results[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(example_image)\n",
    "print(f'The person {\"with\" if is_masked(example_image) else \"without\"} mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Practice <a name=\"tasks\"></a>\n",
    "\n",
    "1. [Task 1: Count number of people with mask](#task1) \n",
    "2. [Task 2: Add blurring logic to predefined video processor](#task2)\n",
    "3. [Task 3: Replace each face in the image with the corresponding emoji](#task3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 1: Count number of people with mask <a name=\"task1\"></a>\n",
    "\n",
    "The goal of this task is to count all faces with masks detected on the photo. We prepared the function to process detected faces. You need to fill out the parts of code to cut a face from the image, blur it, and paste the blurry face image to the original photo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Define the function for inference\n",
    "\n",
    "This function will prepare an image for inference (use `pre_process_input_image` for this) and run inference of the image using  `face_detector`. Note that we will also use this function later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def face_detector_inference(image: np.ndarray) -> np.ndarray:\n",
    "    # 1. Prepare the image\n",
    "    input_frame = pre_process_input_image(image, \n",
    "                                          target_width=face_detector_input_width, \n",
    "                                          target_height=face_detector_input_height)\n",
    "\n",
    "    # 2. Infer the model\n",
    "    face_detection_inference_results = face_detector.infer_new_request({\n",
    "        face_detector_input_name: input_frame\n",
    "    }) \n",
    "    # 3. Get the output of the face detector\n",
    "    face_detector_output = face_detector.output()\n",
    "    return face_detection_inference_results[face_detector_output]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Infer the model on the given image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_result = face_detector_inference(original_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define the function to count masked people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_masked_people(face_detection_inference_results: np.ndarray, original_image: np.ndarray) -> int:\n",
    "    masked_people_counter = 0\n",
    "    # Get the original image size  \n",
    "    original_image_height, original_image_width,  _ = original_image.shape\n",
    "    \n",
    "    # 1. Parse the face detection inference results\n",
    "    detected_faces = parse_face_detection_results(face_detection_inference_results, \n",
    "                                                  original_image_height, original_image_width)\n",
    "    \n",
    "    # 2. Iterate through the faces and blur each face\n",
    "    for detected_face in detected_faces:\n",
    "        xmin, ymin, xmax, ymax, _ = detected_face \n",
    "        # Get the face from the original image \n",
    "        face = original_image[ymin:ymax, xmin:xmax]\n",
    "        if is_masked(face):\n",
    "            masked_people_counter +=1\n",
    "            \n",
    "    return masked_people_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run face detector inference and blur faces in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Count masked faces in the image\n",
    "number_of_masked_people = count_masked_people(inference_result, original_image)\n",
    "show_image(original_image)\n",
    "print(f'Number of masked people is {number_of_masked_people}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 2: Highlight masked and unmasked people <a name=\"task2\"></a>\n",
    "\n",
    "Now that we have learned how to count faces with masked in an image, let's try to find the person who don't wear mask (as the mask detector thinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define the function to draw bounding boxes in image for detexted faces.\n",
    "The boxes will be green if the detected face is masked and red if not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes_of_detected_faces(detected_faces: list, original_image: np.ndarray) -> np.ndarray:\n",
    "    processed_image = original_image.copy()\n",
    "    \n",
    "    # Get output results\n",
    "    green_color = (0, 255, 0)\n",
    "    red_color = (0, 0, 255)\n",
    "    \n",
    "    # Loop through all possible results\n",
    "    for detected_face in detected_faces:\n",
    "        xmin, ymin, xmax, ymax, confidence = detected_face\n",
    "        \n",
    "        face_frame = original_image[ymin:ymax, xmin:xmax]\n",
    "        \n",
    "        mask_detection_results = mask_detector_inference(face_frame)\n",
    "        \n",
    "        masked = int(mask_detection_results[0][0]*100)\n",
    "        \n",
    "        color = green_color if masked > 0.5 else red_color\n",
    "        \n",
    "        # Draw the box and label for the detected object\n",
    "        cv2.rectangle(processed_image, (xmin, ymin), (xmax, ymax), color, 4)\n",
    "        cv2.putText(processed_image, \n",
    "                    f'{masked:.2f}%', (xmin, ymin - 7), \n",
    "                    cv2.FONT_HERSHEY_COMPLEX, 1, color, 2)\n",
    "            \n",
    "    return processed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_faces = parse_face_detection_results(face_detection_inference_results, original_image_height, original_image_width)\n",
    "processed_image = draw_boxes_of_detected_faces(detected_faces, original_image)\n",
    "\n",
    "show_image(processed_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 3: Treack mask on the video <a name=\"task3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define the paths to the input and output videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input video file\n",
    "input_video_path = 'data/input.mp4'\n",
    "\n",
    "# Output video file\n",
    "output_video_path = 'data/output.mp4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Show the input video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the input video\n",
    "HTML(f\"\"\"<center><video width=\"600\" height=\"400\" controls><source src=\"{input_video_path}\" type=\"video/mp4\"></video></center>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create a function to prepare video streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_video_streams(input_video_file_path: str, \n",
    "                          output_video_file_path: str) -> Tuple[cv2.VideoCapture, cv2.VideoWriter]:\n",
    "    input_video_stream = cv2.VideoCapture(input_video_file_path, cv2.CAP_FFMPEG)\n",
    "    width  = int(input_video_stream.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(input_video_stream.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = input_video_stream.get(cv2.CAP_PROP_FPS)\n",
    "    video_writer = cv2.VideoWriter(output_video_file_path, cv2.VideoWriter_fourcc(*'avc1'), fps, (width, height))\n",
    "    return input_video_stream, video_writer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Process the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video_stream, output_video_stream = prepare_video_streams(input_video_path, output_video_path)\n",
    "\n",
    "while input_video_stream.isOpened():\n",
    "    # 1. Read the next frame from the input video \n",
    "    return_code, original_frame = input_video_stream.read()\n",
    "    if not return_code:\n",
    "        break\n",
    "    \n",
    "    # 2. Get \n",
    "    original_frame_height, original_frame_width,  _ = original_frame.shape\n",
    "    \n",
    "    # 2. Infer face detector\n",
    "    inference_results = face_detector_inference(original_frame)\n",
    "    \n",
    "    # 2. Parse coordinates of the detected faces\n",
    "    detected_faces = parse_face_detection_results(inference_results, original_frame_height, original_frame_width)\n",
    "    \n",
    "    # 3. Blur faces in the image\n",
    "    processed_image = draw_boxes_of_detected_faces(detected_faces, original_frame)\n",
    "    \n",
    "    # 4. Write the resulting frame to the output stream\n",
    "    output_video_stream.write(processed_image)\n",
    "    \n",
    "\n",
    "input_video_stream.release()\n",
    "# Save the resulting video\n",
    "output_video_stream.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Show the resulting video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(f\"\"\"<video width=\"600\" height=\"400\" controls><source src=\"{output_video_path}\" type=\"video/mp4\"></video>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pictures/recap.png\" />\n",
    "\n",
    "#### Workshop repository: [https://github.com/dl-wb-experiments/face-hiding-workshop](https://github.com/dl-wb-experiments/face-hiding-workshop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Next steps with OpenVINO <a name=\"next-steps\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Try Workbench right now: locally or in DevCloud\n",
    "\n",
    "![](pictures/call-to-action.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. [Intel® Edge AI Certification](https://www.intel.com/content/www/us/en/developer/tools/devcloud/edge/learn/certification.html)\n",
    "\n",
    "![](pictures/edge-ai-certification.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
