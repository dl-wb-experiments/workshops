{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</figure>\n",
    "<center> <h1>Сreating an application for automatic detection of toxic comments using transformers and OpenVINO™</h1> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./nlp_img/twitter_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. [Introduction](#intro)\n",
    "\n",
    "## 2. [OpenVINO™ Overview](#OV-overview)\n",
    "\n",
    "## 3. [OpenVINO™ Deep Learning Workbench](#DL-WB-overview)\n",
    "\n",
    "## 4. [OpenVINO™ API](#OV-API)\n",
    "\n",
    "## 5. [Practice](#tasks)\n",
    "\n",
    "## 6. [Bonus: Next steps with OpenVINO](#next-steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction  <a name=\"intro-nlp\"></a>\n",
    "\n",
    "### Workshop Contributors\n",
    "\n",
    "<div style=\"display: block; text-align: center;\">\n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img style=\"border-radius: 50%;\" src=\"nlp_img/Demidovskij.jpg\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Alexander Demidovskij</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://github.com/demid5111\" style=\"display: inline-flex;\"><img src=\"./nlp_img/github.svg\" width=\"22px\" style=\"margin-right: 5px\">@demid5111</a>\n",
    "      </figcaption>\n",
    "    </figure>\n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img style=\"border-radius: 50%;\" src=\"nlp_img/Paniukov.jpg\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Artur Paniukov</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://github.com/apaniukov\" style=\"display: inline-flex;\"><img src=\"./nlp_img/github.svg\" width=\"22px\" style=\"margin-right: 5px\">@apaniukov</a>\n",
    "      </figcaption>\n",
    "      </figure>\n",
    "    <br>\n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img style=\"border-radius: 50%;\" src=\"nlp_img/Savina.jpg\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Tatiana Savina</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://github.com/tsavina\" style=\"display: inline-flex;\"><img src=\"./nlp_img/github.svg\" width=\"22px\" style=\"margin-right: 5px\">@tsavina</a>\n",
    "      </figcaption>\n",
    "    </figure>    \n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img style=\"border-radius: 50%;\" src=\"nlp_img/Salnikov.jpg\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Igor Salnikov</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://github.com/SalnikovIgor\" style=\"display: inline-flex;\"><img src=\"./nlp_img/github.svg\" width=\"22px\" style=\"margin-right: 5px\">@SalnikovIgor</a>\n",
    "      </figcaption>\n",
    "    </figure>\n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img style=\"border-radius: 50%;\" src=\"nlp_img/Tugaryov.jpg\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Artyom Tugaryov</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://github.com/artyomtugaryov\" style=\"display: inline-flex;\"><img src=\"./nlp_img/github.svg\" width=\"22px\" style=\"margin-right: 5px\">@artyomtugaryov</a>\n",
    "      </figcaption>\n",
    "    </figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### In This Workshop\n",
    "\n",
    "Welcome to the Deep Learning workshop, where you will find out how to start working with pre-trained neural networks and create your own  AI application. For that, we will be using OpenVINO™ framework and its graphical interface Deep Learning Workbench. \n",
    "\n",
    "During this workshop, you will:\n",
    "\n",
    "1. Learn the basics of neural model analysis and optimization:\n",
    "    - what a model is and how it works\n",
    "    - how model inference works\n",
    "    - how to measure model performance \n",
    "    - how to tune the model for enhanced performance\n",
    "2. Write your own AI application that detects toxic messages.\n",
    "\n",
    "###  NLP and Deep Learning\n",
    "\n",
    "Natural Language Processing (NLP) is a branch of technology that helps programs understand and interpret human language. NLP fills the gap between human communication and machine comprehension. With the rapid development of the NLP field, Deep Learning models achieve state-of-the-art performance in various business use cases. According to IBM's [Global AI Adoption Index 2021](https://newsroom.ibm.com/IBMs-Global-AI-Adoption-Index-2021?lnk=ushpv18ai3), about half of companies are already employing NLP-enabled applications, and more than a quarter plan to do so within the following year. \n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img src=\"nlp_img/dynabench_plot.png\" style=\"height: 300px;max-width: initial;\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Benchmark saturation over time for popular benchmarks. Initial performance and human performance are normalised to -1 and 0 respectively</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://aclanthology.org/2021.naacl-main.324.pdf\">Source</a>\n",
    "      </figcaption>\n",
    "    </figure>\n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img src=\"nlp_img/graph2.png\" style=\"height: 300px;max-width: initial;\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Humans vs. Neural Models (SuperGLUE)</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://super.gluebenchmark.com/leaderboard\">Source</a>\n",
    "      </figcaption>\n",
    "    </figure>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "Typically, NLP algorithms evaluate vast amounts of unstructured text data, such as documents, log files, transcripts, etc. The output of an NLP model might vary based on its objective. Digital Assistants like Alexa (Amazon), Siri (Apple), Cortana (Microsoft), and Google Assistant use NLP to identify speech patterns and infer meaning or complete a task to assist the user. Recent advances have enabled the NLP field to evolve from tools like spell check and spam filter to more sophisticated apps such as NLP-powered chatbots, real-time voice-to-text translators, and other services that can conduct phone calls, track conversations and collect important data from chats and emails, and many more.\n",
    "\n",
    "Along with the achieved groundbreaking results, the size of these models has skyrocketed, reaching millions (or even billions) of parameters, combined with increasing growth in complexity. Therefore, there is a strong need to accelerate neural models inference so that they can meet demanding performance and accuracy requirements in production. Before we try one of the acceleration techniques, let's define the model inference and find out how it differs from the model training. \n",
    "\n",
    "###  Text Model Inference\n",
    "\n",
    "To perform a specific AI task, the model is trained on the known data. During training, the model makes predictions about what the data represents. Any error in the prediction is used to strengthen the artificial neuron connections until the acceptable accuracy level is achieved. The process of training usually done just once and requires massive amount of data and powerful computing systems.\n",
    "\n",
    "The process of using a pre-trained model to make predictions against previously unseen data is called inference. Inference is performed on different devices, happens several times, and consists in feeding the data to the model. In other words, with NLP technologies inferences happen almost all the time, and model output is used by companies and apps for speech recognition, search autocomplete, spam filter, recommendation systems, etc. \n",
    "\n",
    "![](nlp_img/inference_example.png)\n",
    "\n",
    "In this workshop we will work with model inference; in other words, we will execute the model already pre-trained to perform useful work. Let's take a look at the inference on the real-life example of the toxic message detection model.\n",
    "\n",
    "Web services and apps that handle large numbers of user messages might profit from automated moderation systems. Potentially, such systems could reduce the cost and time required for manual moderation while increasing the process efficiency. Suppose we have online chats where messages are created by hundreds of users every minute. The messages are processed on the server, and the task of our NLP model is to filter out potentially toxic communications before they reach the chat interface. The model must respond quickly enough so that the structure of the users' conversation is not disrupted.\n",
    "\n",
    "In this case, inference consists in: \n",
    "\n",
    "-\ttaking a text message as input\n",
    "-\ttext preprocessing\n",
    "-\tfeeding that data to our model\n",
    "-\treturning a classification label (toxic/non-toxic) produced by the model\n",
    "\n",
    "\n",
    "Besides making NLP models fast enough for users, we need to accelerate them and make economical enough to run in production at a manageable cost. As a result, there is a challenge of accelerating inference of usually complex and large NLP models in the absence of any expensive specialized hardware.\n",
    "\n",
    "So let's find out how the OpenVINO™ toolkit can help in achieving better performance of NLP models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. OpenVINO™ Toolkit <a name=\"OV-overview\"></a>\n",
    "\n",
    "1. [How OpenVINO advances AI technologies](#customers) \n",
    "2. [Introduction to the Openvino toolkit](#OV-intro) \n",
    "3. [OpenVINO capabilities](#OV-capabilities)\n",
    "4. [OpenVINO components](#OV-components)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 How OpenVINO advances AI technologies <a name=\"customers-nlp\"></a>\n",
    "\n",
    "The Open Visual Inference & Neural Network Optimization (OpenVINO™) toolkit is a comprehensive toolkit for optimizing pre-trained Deep Learning models of various use cases to achieve high performance and prepare them for deployment on Intel® platforms. Based on latest generations of artificial neural networks, including convolutional neural networks (CNNs), recurrent and attention-based networks, the toolkit extends computer vision and non-vision workloads across Intel® hardware, maximizing performance. It accelerates applications with high-performance, AI and deep learning inference deployed from edge to cloud.\n",
    "\n",
    "![](nlp_img/sales-collage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Introduction to the OpenVINO™ toolkit <a name=\"OV-intro\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nlp_img/ov_motivation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 OpenVINO™ Capabilities <a name=\"OV-capabilities\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nlp_img/ov_tools.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 OpenVINO™ Toolkit Components <a name=\"OV-components\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nlp_img/additional_tools.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 OpenVINO™ Open Model Zoo <a name=\"OMZ\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nlp_img/omz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deep Learning Workbench: OpenVINO™ Quickstart <a name=\"DL-WB-overview\"></a>\n",
    "\n",
    "1. [DL Workbench Capabilities](#DL-WB-capabilities)\n",
    "2. [DL Workbench Workflow](#DL-WB-workflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning Workbench (DL Workbench) is the official OpenVINO™ graphical interface designed to make the production of pre-trained deep learning models significantly easier.\n",
    "With DL Workbench you can start working with your deep learning model right from your browser: import a model, analyze its performance and accuracy, visualize the outputs, optimize and prepare the model for deployment in a matter of minutes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 DL Workbench Capabilities <a name=\"DL-WB-capabilities\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"nlp_img//DL-WB-flow.png\" width=\"1400\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 DL Workbench Workflow <a name=\"DL-WB-workflow-nlp\"></a>\n",
    "\n",
    "\n",
    "1. [Open DL Workbench](#open-wb-nlp)\n",
    "2. [Import the Model](#import-model-nlp)\n",
    "3. [Import the Dataset](#import-dataset-nlp) \n",
    "4. [Benchmark the Model](#model-inference-nlp)\n",
    "5. [Analyze the Model](#analyze-model-nlp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display button for opening DL Workbench\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"<button class=\"wb-button\" onclick=\"window.open(location.origin, '_blank');\">Open DL Workbench</button><style>.wb-button { display: flex; width: fit-content; margin: 20px auto; align-items: center; height: 50px; font-size: 18px; font-weight: 400; font-family: inherit; line-height: 20px; background-color: #003C71; border: 1px solid #003C71; color: #ffffff; cursor: pointer; border-radius: 4px; padding: 0 30px; box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.15);}</style>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open DL Workbench <a name=\"open-wb-nlp\"></a>\n",
    "\n",
    "To start working with DL Workbench, click **Create Project** button to open the Create Project page.\n",
    "\n",
    "![](nlp_img/create_project_clear.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Model <a name=\"import-model-nlp\"></a>\n",
    "\n",
    "Our first step is to **Import the model**. \n",
    "\n",
    "![](nlp_img/import_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select **Original Model** tab, specify **NLP** domain and **ONNX** framework. \n",
    "\n",
    "![](nlp_img/import_nlp_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install requirements and activate ipywidgets for the transformers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download [russian_toxicity_classifier](https://huggingface.co/chgk13/tiny_russian_toxic_bert) model from HuggingFace Hub and convert it to ONNX format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from shutil import rmtree\n",
    "\n",
    "from transformers.convert_graph_to_onnx import convert\n",
    "\n",
    "model_checkpoint = \"chgk13/tiny_russian_toxic_bert\"\n",
    "model_name = model_checkpoint.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = Path(f\"onnx_model/{model_name}.onnx\")\n",
    "if onnx_model_path.parent.exists():\n",
    "    rmtree(onnx_model_path.parent)\n",
    "\n",
    "convert(\n",
    "    framework=\"pt\",\n",
    "    model=model_checkpoint,\n",
    "    output=onnx_model_path,\n",
    "    opset=12,  # check other opsets or try one more time if conversion fails\n",
    "    pipeline_name=\"sentiment-analysis\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select and upload `.onnx` model file and click **Import**: \n",
    "![](nlp_img/d_import_wizard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with OpenVINO tools, you need to obtain a model in Intermediate Representation (IR) format. \n",
    "IR is the OpenVINO format of pre-trained model representation with two files:\n",
    "\n",
    "- XML file describing the network topology\n",
    "- BIN file containing weights and biases\n",
    "\n",
    "On the third stage **Convert Model to IR - General Parameters**, click **Convert** to proceed:\n",
    "\n",
    "![](nlp_img/d_convert_model.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layout describes the value of each dimension of input tensors. To configure model layout, set **NC layout**. N is the size of a batch showing how many texts the model processes at a time. C is the maximum length of text (in tokens) that our model can process.\n",
    "Specify **1 batch** and **128 tokens** (channels) for each input. Click **Validate and Import**:\n",
    "\n",
    "![](nlp_img/d_layput.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model is successfully imported, you will see it on the Create Project page. Click on the model to select it:\n",
    "\n",
    "![](nlp_img/d_model_ready.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proceed and click **Next Step**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Environment <a name=\"import-dataset-nlp\"></a>\n",
    "\n",
    "On the **Select Environment** page you can choose a hardware accelerator on which the model will be executed. We will analyze our model on a CPU since we have only this device available. Go to the **Next Step**.\n",
    "\n",
    "![](nlp_img/d_select_env.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset <a name=\"import-dataset-nlp\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation of the model is always performed against specific data combined into datasets. You will need to obtain the data to work with the model. The data depends on the task for which the model has been trained. Learn more in the [documentation](https://docs.openvinotoolkit.org/latest/workbench_docs_Workbench_DG_Dataset_Types.html). \n",
    "\n",
    "In our case, we take a set of toxic and non-toxic messages and use them to create our validation dataset.\n",
    "\n",
    "Use the following **link to download the dataset**: [Download Dataset](https://github.com/dl-wb-experiments/workshops/files/7951354/dataset.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the **Select Validation Dataset** page, click **Import Text Dataset**.\n",
    "\n",
    "![](nlp_img/d_text_dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the dataset file. Specify **UTF-8** encoding and **Comma** as separator. In the **Raw Dataset Preview** you can see that our dataset **Has Header**. Select **Text Classification** task type and **Text Column - 1, Label Column - 2** . Make sure the dataset is displayed correctly in the **Formated Dataset Preview** and click **Import**.\n",
    "\n",
    "![](nlp_img/d_text_preview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark the Model <a name=\"model-inference-nlp\"></a>\n",
    "\n",
    "Now that we have imported our model, we want to check how fast it works. For that, let's create our first project. Select the model and the dataset by clicking on them, and click **Create Project**.\n",
    "\n",
    "![](nlp_img/create_project_completed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the Model <a name=\"analyze-model-nlp\"></a>\n",
    "\n",
    "When the inference stage is finished, we can see the result of running our model on the CPU. \n",
    "\n",
    "![](nlp_img/d_benchmark_results.png)\n",
    "\n",
    "Latency is the time required to process the data. The lower the value, the better. Throughput shows how much data the model processes per second. Higher throughput value means better performance. \n",
    "\n",
    "Under the table with results you see a hint saying the model was inferred on the autogenerated data. To infer the model on the text dataset, you need to use a tokenizer. Click **Import Tokenizer** link in the hint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Tokenizer <a name=\"import-dataset-nlp\"></a>\n",
    "\n",
    "Tokenizers are used to convert text to numerical data because the model cannot work with the text directly. Tokenizer splits text into tokens. A token can be a word,\n",
    "part of a word, a symbol, or a couple of symbols. Then tokenizer\n",
    "replaces each token with the corresponding index and stores\n",
    "the map between tokens and indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the cells to download a tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "tokenizer.save_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To benchmark our model on the text from the imported dataset, we need to:\n",
    "\n",
    "#### Import tokenizer\n",
    "\n",
    "Select **WordPiece** as tokenizer type, upload `vocab.txt` file, and click **Import**.\n",
    "\n",
    "![](nlp_img/d_import_tokenizer.png)\n",
    "\n",
    "#### Select tokenizer\n",
    "\n",
    "Click on the tokenizer to select it.\n",
    "\n",
    "![](nlp_img/d_select_tokenizer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profile the Model <a name=\"profile-model-nlp\"></a>\n",
    "\n",
    "\n",
    "Open the **Project** tab and go back to the model project:\n",
    "\n",
    "![](nlp_img/d_project.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Select **Perform** and open **Explore Inference Configurations** tab. \n",
    "\n",
    "You can accelerate your model by configuring the optimal inference parameters specific to each accelerator: streams and batches.\n",
    "Streams are the number of instances of your model running simultaneously, and batches are the number of input data instances fed to the model. Let's first infer our model with default **1 Stream** and **1 Batch** parameters.\n",
    "\n",
    "![](nlp_img/d_perform.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also run a range of inference streams to find the optimal configurations and accelerate your model. Go to the **Perform** tab, open **Explore Inference Configurations** subtab, select Group Inference and click **Configure Group Inference**. On the Configure Group Inference page, select combinations of stream and batch parameters by clicking corresponding cells in the table and click **Execute**.\n",
    "\n",
    "![](nlp_img/d_execute.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results. Remember that higher throughput value means better performance. \n",
    "\n",
    "![](nlp_img/d_benchmark_results.png)\n",
    "\n",
    "Our model became X time faster with X batch X stream configuration. Furhter, we will use this information to create our working application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s briefly recap what you have learned at this stage:\n",
    "\n",
    "1. What a model is and how it works\n",
    "2. What model inference is and why it is important to accelerate its speed\n",
    "3. How to measure and improve model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Learn OpenVINO™ API <a name=\"OV-API\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Let's learn how to infer a model of text classification use case with OpenVINO™ Python interface and build our application.\n",
    "\n",
    "We will go through the following steps:\n",
    "\n",
    "1. [Import required modules](#Import) \n",
    "2. [Configure inference: path to a model and other data](#Configure)\n",
    "3. [Initialize the OpenVINO™ runtime](#Initialize)\n",
    "4. [Read the model](#Read)\n",
    "5. [Compile the Model for the Device](#Compile)\n",
    "6. [Prepare a text for model inference](#Prepare)\n",
    "7. [Infer the model](#Infer)\n",
    "8. [Show predictions](#Show)\n",
    "9. [Combine inference and result processing](#Combine)\n",
    "10. [Predict labels for a dataset](#Dataset)\n",
    "10. [Make the model dynamic](#Dynamic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Import Required Modules <a name=\"Import\"></a>\n",
    "\n",
    "Import the Python* modules that you will use in the sample code:\n",
    "- [OpenVINO](https://docs.openvinotoolkit.org/latest/openvino_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html) is an OpenVINO™ Python API module used for inference\n",
    "- [NumPy](http://www.numpy.org/) is an array manipulation module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from openvino.runtime import Core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Configure Inference <a name=\"Configure\"></a>\n",
    "\n",
    "Once you have the OpenVINO™ IR of your model, you can start experimenting with it by inferring it and inspecting its output. \n",
    "\n",
    "> **NOTE**: If you have the model imported in DL Workbench, copy the paths to the `.xml` and `.bin` files from the DL Workbench UI and paste them below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements\n",
    "\n",
    "Parameter| Explanation\n",
    "---|---\n",
    "**model_xml**| Path to the `.xml` file of OpenVINO™ IR of your model\n",
    "**model_bin**| Path to the `.bin` file of OpenVINO™ IR of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model IR files\n",
    "model_xml = Path(\"ir_model/\") / f\"{model_name}.xml\"\n",
    "model_bin = Path(\"ir_model/\") / f\"{model_name}.bin\"\n",
    "\n",
    "# Path to datset file\n",
    "dataset_path = \"dataset.csv\"\n",
    "\n",
    "# Max sequence length for model in tokens\n",
    "sequence_length = 128\n",
    "\n",
    "# Device to use\n",
    "device = \"CPU\"\n",
    "\n",
    "print(\n",
    "    \"Configuration parameters settings:\"\n",
    "    f\"\\n\\tmodel_xml={model_xml}\",\n",
    "    f\"\\n\\tmodel_bin={model_bin}\",\n",
    "    f\"\\n\\tdataset_path={dataset_path}\"\n",
    "    f\"\\n\\tsequence_length={sequence_length}\"\n",
    "    f\"\\n\\tdevice={device}\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Initialize the OpenVINO™ Runtime <a name=\"Initialize\"></a>\n",
    "\n",
    "Once you define the parameters, let's initiate the `Core` object that accesses OpenVINO™ runtime capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Inference Engine instance\n",
    "core = Core()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Read the Model <a name=\"Read\"></a>\n",
    "\n",
    "Put the IR of your model in the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the network from IR files\n",
    "model = core.read_model(model=model_xml, weights=model_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Compile the Model for the Device <a name=\"Compile\"></a>\n",
    "\n",
    "Reading a network is not enough to start a model inference. The model must be loaded to a particular abstraction representing a particular accelerator. In OpenVINO™, this abstraction is called *plugin*. A network loaded to a plugin becomes executable and will be inferred in one of the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = core.compile_model(model=model, device_name=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Prepare a Text for Model Inference <a name=\"Prepare\"></a>\n",
    "\n",
    "The model cannot work with the text directly. Instead, the text is first split into tokens and then replaces each token with the corresponding index. A token can be a word, part of a word, a symbol, or a couple of symbols. The map between tokens and indices is stored by a tokenizer.\n",
    "\n",
    "The model input that takes token indices is usually called `input_ids`. There are also might be other inputs. If the model input is static, meaning that it can take only fixed-size inputs, one could have to make an input text longer. For that, there is a special token, called _padding_, that can be added to the beginning or to the end of the sequence. To ignore these tokens during inference there is an `attention_mask` model input.\n",
    "\n",
    "The third part of the tokenized text called `token_type_ids`. If you want to pass a pair of text to the model, token type will separate the first text from the second one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = [\"Мне фильм понравился\"]\n",
    "\n",
    "tokenized_text = tokenizer(\n",
    "    input_text,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    pad_to_multiple_of=sequence_length,\n",
    "    return_tensors=\"np\",\n",
    ")\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Infer the Model <a name=\"Infer\"></a>\n",
    "\n",
    "OpenVINO runs inference via `inference_request` objects. The object contains the input data and the inference result. One can create this object separately, set the input data to it and get the inference result afterward. But we will use the simpler `infer_new_request` method to do all this for us.\n",
    "\n",
    "The `tokenized_text` is a dict-like object. To pass it to the network, we should transform it to a python dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the starting time\n",
    "inf_start = time.time()\n",
    "\n",
    "# Run the inference\n",
    "tokenized_text_dict = dict(**tokenized_text)\n",
    "res = compiled_model.infer_new_request(tokenized_text_dict)\n",
    "\n",
    "# Calculate the time from the start until now\n",
    "inf_time = time.time() - inf_start\n",
    "print(f\"Inference is complete. Run time: {inf_time * 1000:.3f} ms.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Show Predictions <a name=\"Show\"></a>\n",
    "\n",
    "Now `res` contains the result of the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig\n",
    "\n",
    "\n",
    "# try to load index to label map from model config\n",
    "model_config = PretrainedConfig.from_pretrained(model_checkpoint)\n",
    "idx_to_label_dict = getattr(model_config, \"id2label\", {})\n",
    "\n",
    "idx_to_label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function that can be applied to vectors\n",
    "# if there is no `idx_to_label` map, the function returns class index\n",
    "idx_to_label = np.vectorize(lambda x: idx_to_label_dict.get(x, x))\n",
    "\n",
    "\n",
    "def process_result(res, compiled_model=compiled_model):\n",
    "    # get model output\n",
    "    res = res[compiled_model.output()]\n",
    "    \n",
    "    # take the position of the maximum element in the vector to get the predicted class index\n",
    "    predicted_class_idx = np.argmax(res, axis=1)\n",
    "    \n",
    "    # transform class index to class label\n",
    "    return idx_to_label(predicted_class_idx)\n",
    "\n",
    "process_result(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 Combine Inference and Result Processing <a name=\"Combine\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    tokenized_text = tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        pad_to_multiple_of=sequence_length,\n",
    "        return_tensors=\"np\",\n",
    "    )\n",
    "    res = compiled_model.infer_new_request(dict(**tokenized_text))\n",
    "    logits = res[compiled_model.output()]\n",
    "    predicted_class_idx = np.argmax(logits, axis=1)\n",
    "    return idx_to_label(predicted_class_idx)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predict(\"Шел бы ты отсюда...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.10 Predict Labels for a Dataset <a name=\"Dataset\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "dataset = pd.read_csv(dataset_path, names=[\"text\", \"label\"])\n",
    "dataset.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset[\"predicted\"] = dataset[\"text\"].apply(predict)\n",
    "dataset.style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.11 Make the Model Dynamic <a name=\"Dynamic\"></a>\n",
    "\n",
    "The transformer model has computational complexity $O(n^2)$, where $n$ is the length of the sequence. So, if one increase the length of the sequence with padding, extra calculations will occur and by increasing the length twice, the time might increace by the factor of four. To reduce padding or even eliminate it, one can use a dynamic shape model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "current_model_shape = {\n",
    "    input_.any_name: input_.shape\n",
    "    for input_ in model.inputs\n",
    "}\n",
    "current_model_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.runtime import PartialShape\n",
    "\n",
    "\n",
    "new_model_shape = {\n",
    "    input_.any_name: PartialShape([-1, -1])\n",
    "    for input_ in model.inputs\n",
    "}\n",
    "new_model_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reshape(new_model_shape)\n",
    "dynamic_model = core.compile_model(model=model, device_name=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the starting time\n",
    "inf_start = time.time()\n",
    "\n",
    "# Run the inference\n",
    "res = dynamic_model.infer_new_request(dict(**tokenized_text))\n",
    "\n",
    "# Calculate the time from the start until now\n",
    "inf_time = time.time() - inf_start\n",
    "print(f\"Inference is complete. Run time: {inf_time * 1000:.3f} ms.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dynamic model is slower than the static on the same input, but now we can reduce the input length and pass the contracted data to the dynamic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = [\"Мне фильм понравился\"]\n",
    "\n",
    "tokenized_text = tokenizer(\n",
    "    input_text,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"np\",\n",
    ")\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the starting time\n",
    "inf_start = time.time()\n",
    "\n",
    "# Run the inference\n",
    "res = dynamic_model.infer_new_request(dict(**tokenized_text))\n",
    "\n",
    "# Calculate the time from the start until now\n",
    "inf_time = time.time() - inf_start\n",
    "print(f\"Inference is complete. Run time: {inf_time * 1000:.3f} ms.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_result(res, dynamic_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Now you can proceed to importing the model into the DL Workbench or if you have already done that, start exploring numerous features such as:\n",
    "\n",
    "* [Analyse how the model works and its quality](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Visualize_Accuracy.html)\n",
    "* [Perform a baseline inference and analyze model performance](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Run_Single_Inference.html)\n",
    "* [Boost the model by calibrating it to the INT8 precision](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Int_8_Quantization.html)\n",
    "* [Tune the performance of the model by selecting optimal inference parameters](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Run_Range_of_Inferences.html)\n",
    "* [Preparing the model for deployment](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Deploy_and_Integrate_Performance_Criteria_into_Application.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice <a name=\"tasks\"></a>\n",
    "\n",
    "1. [Task 1: ](#task1) analyze toxicity of the most popular tweets of the users\n",
    "2. [Task 2: ](#task2) filter a user followings from a toxic users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1:  <a name=\"task1\"></a>\n",
    "\n",
    "The goal of this task is to scrape a data from tweeter and analyze a toxicity of a tweeter users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import twint\n",
    "import nest_asyncio\n",
    "\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to create a config for the search. A [list](https://github.com/twintproject/twint/wiki/Configuration) of all configuration options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = twint.Config()\n",
    "config.Limit = 20  # increment of 20\n",
    "# config.Hide_output = True  # print tweets to stdout\n",
    "config.Username = None\n",
    "config.Search = \"opentalksai\"  # search terms\n",
    "config.Pandas = True  # save data to pandas DataFrame\n",
    "config.Filter_retweets = True\n",
    "\n",
    "twint.run.Search(config)\n",
    "tweets_df = twint.storage.panda.Tweets_df\n",
    "tweets_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check what data we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.sample(1).style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also display a tweet via a link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import requests\n",
    "\n",
    "def show_tweet(link):\n",
    "    url = f\"https://publish.twitter.com/oembed?url={link}\"\n",
    "    response = requests.get(url)\n",
    "    html = response.json()[\"html\"]\n",
    "    display(HTML(html))\n",
    "    \n",
    "sample_tweet_link = tweets_df.sample(1).iloc[0].link\n",
    "show_tweet(sample_tweet_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap a twint into a function\n",
    "\n",
    "Create a function with a giving signature that returns a pandas DataFrame with scraped tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tweets(\n",
    "    search=None,\n",
    "    username=None,\n",
    "    limit=20,\n",
    "    since=None,  # yyyy-mm-dd\n",
    "    until=None,  # yyyy-mm-dd\n",
    "    filter_retweets=True,\n",
    "    popular_tweets=False\n",
    "):\n",
    "    # create a twint config object\n",
    "    ...\n",
    "    \n",
    "    # pass a searching parameters\n",
    "    ...\n",
    "    \n",
    "    # run twint search\n",
    "    \n",
    "    # return a DataFrame with twitter data\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tweets(\n",
    "    search=None,\n",
    "    username=None,\n",
    "    limit=20,\n",
    "    since=None,  # yyyy-mm-dd\n",
    "    until=None,  # yyyy-mm-dd\n",
    "    filter_retweets=True,\n",
    "    popular_tweets=False\n",
    "):\n",
    "    config = twint.Config()\n",
    "    \n",
    "    config.Limit = limit\n",
    "    config.Hide_output = True\n",
    "    config.Username = username\n",
    "    config.Search = search\n",
    "    config.Pandas = True\n",
    "    config.Filter_retweets = filter_retweets\n",
    "    config.Since = since\n",
    "    config.Until = until\n",
    "    config.Popular_tweets = popular_tweets\n",
    "\n",
    "    twint.run.Search(config)\n",
    "    \n",
    "    return twint.storage.panda.Tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a fraction of user's toxic tweets\n",
    "\n",
    "Let's rank a couple of famous Twitter users by the number of toxic tweets. First, we need a function that counts the number of toxic and neutral tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_twitter_user_toxic_tweets(username, number_of_tweets):\n",
    "    # scrape the most popular tweets of the user\n",
    "    tweets = scrape_tweets(\n",
    "        username=username,\n",
    "        limit=number_of_tweets,\n",
    "        popular_tweets=True\n",
    "    )\n",
    "    \n",
    "    # apply a toxicity classifier\n",
    "    tweets.toxicity = tweets.tweet.apply(predict)\n",
    "    \n",
    "    # count a number of tweets for each class\n",
    "    tweets_count = tweets.toxicity.value_counts()\n",
    "    \n",
    "    # convert the result to dict and return it\n",
    "    return tweets_count.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the data for some twitter accounts and count a number of toxic and non-toxic tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usernames = [\n",
    "    \"vas3k\",\n",
    "    \"VRSoloviev\",\n",
    "    \"Zhirinovskiy\",\n",
    "]\n",
    "number_of_tweets = 40\n",
    "\n",
    "results = {}\n",
    "for username in usernames:\n",
    "    # get a tweets count and save it to the result dictionary \n",
    "    tweets_count = count_twitter_user_toxic_tweets(username)\n",
    "    results[user] = tweet_count\n",
    "    \n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a fraction of toxic tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a pie chart we will use a `plt.pie`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in pie_data.itertuples():\n",
    "    print(row[0])\n",
    "    plt.pie(row[1:], labels=results_df.columns , autopct = '%0.0f%%')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2:  <a name=\"task2\"></a>\n",
    "\n",
    "Now that we have learned how to count, let's try to create an application, that filters a user's following list from toxic twitter accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = twint.Config()\n",
    "c.Username = \"Zhirinovskiy\"\n",
    "c.Pandas = True\n",
    "\n",
    "twint.run.Following(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Next steps with OpenVINO <a name=\"next-steps\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Try Workbench right now: locally or in DevCloud\n",
    "\n",
    "![](workshops/cv/pictures/call-to-action.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. [Intel® Edge AI Certification](https://www.intel.com/content/www/us/en/developer/tools/devcloud/edge/learn/certification.html)\n",
    "\n",
    "![](pictures/edge-ai-certification.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "186.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}