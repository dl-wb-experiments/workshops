{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</figure>\n",
    "<center> <h1>Сreating an application for automatic detection of toxic comments using transformers and OpenVINO™</h1> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"./nlp_img/toxic_twitter_image_1.png\" width=\"1150\" height=\"650\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. [Introduction](#intro)\n",
    "\n",
    "## 2. [OpenVINO™ Overview](#OV-overview)\n",
    "\n",
    "## 3. [OpenVINO™ Deep Learning Workbench](#DL-WB-overview)\n",
    "\n",
    "## 4. [OpenVINO™ API](#OV-API)\n",
    "\n",
    "## 5. [Practice](#tasks)\n",
    "\n",
    "## 6. [Bonus: Next steps with OpenVINO](#next-steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction  <a name=\"intro-nlp\"></a>\n",
    "\n",
    "### Workshop Contributors\n",
    "\n",
    "<div style=\"display: block; text-align: center;\">\n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img style=\"border-radius: 50%;\" src=\"nlp_img/Demidovskij.jpg\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Alexander Demidovskij</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://github.com/demid5111\" style=\"display: inline-flex;\"><img src=\"./nlp_img/github.svg\" width=\"22px\" style=\"margin-right: 5px\">@demid5111</a>\n",
    "      </figcaption>\n",
    "    </figure>\n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img style=\"border-radius: 50%;\" src=\"nlp_img/Paniukov.jpg\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Artur Paniukov</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://github.com/apaniukov\" style=\"display: inline-flex;\"><img src=\"./nlp_img/github.svg\" width=\"22px\" style=\"margin-right: 5px\">@apaniukov</a>\n",
    "      </figcaption>\n",
    "      </figure>\n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img style=\"border-radius: 50%;\" src=\"nlp_img/Savina.jpg\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Tatiana Savina</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://github.com/tsavina\" style=\"display: inline-flex;\"><img src=\"./nlp_img/github.svg\" width=\"22px\" style=\"margin-right: 5px\">@tsavina</a>\n",
    "      </figcaption>\n",
    "    </figure>  \n",
    "     <br>  \n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img style=\"border-radius: 50%;\" src=\"nlp_img/Salnikov.jpg\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Igor Salnikov</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://github.com/SalnikovIgor\" style=\"display: inline-flex;\"><img src=\"./nlp_img/github.svg\" width=\"22px\" style=\"margin-right: 5px\">@SalnikovIgor</a>\n",
    "      </figcaption>\n",
    "    </figure>\n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img style=\"border-radius: 50%;\" src=\"nlp_img/Tugaryov.jpg\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Artyom Tugaryov</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://github.com/artyomtugaryov\" style=\"display: inline-flex;\"><img src=\"./nlp_img/github.svg\" width=\"22px\" style=\"margin-right: 5px\">@artyomtugaryov</a>\n",
    "      </figcaption>\n",
    "    </figure>\n",
    "        <figure style=\"display: inline-block;\">\n",
    "      <img style=\"border-radius: 50%;\" src=\"nlp_img/Tarkan.jpg\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Yaroslav Tarkan</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://github.com/yatarkan\" style=\"display: inline-flex;\"><img src=\"./nlp_img/github.svg\" width=\"22px\" style=\"margin-right: 5px\">@yatarkan</a>\n",
    "      </figcaption>\n",
    "    </figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### In This Workshop\n",
    "\n",
    "Welcome to the Deep Learning workshop, where you will find out how to start working with pre-trained neural networks and create your own  AI application. For that, we will be using OpenVINO™ framework and its graphical interface Deep Learning Workbench. \n",
    "\n",
    "During this workshop, you will:\n",
    "\n",
    "1. Learn the basics of neural model analysis and optimization:\n",
    "    - what a model is and how it works\n",
    "    - how model inference works\n",
    "    - how to measure model performance \n",
    "    - how to tune the model for enhanced performance\n",
    "2. Write your own AI application that detects toxic messages.\n",
    "\n",
    "###  NLP and Deep Learning\n",
    "\n",
    "Natural Language Processing (NLP) is a branch of technology that helps programs understand and interpret human language. NLP fills the gap between human communication and machine comprehension. With the rapid development of the NLP field, Deep Learning models achieve state-of-the-art performance in various business use cases. According to IBM's [Global AI Adoption Index 2021](https://newsroom.ibm.com/IBMs-Global-AI-Adoption-Index-2021?lnk=ushpv18ai3), about half of companies are already employing NLP-enabled applications, and more than a quarter plan to do so within the following year. \n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img src=\"nlp_img/chart_nlp.png\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Humans vs. Neural Models (SuperGLUE)</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://super.gluebenchmark.com/leaderboard\">Source</a>\n",
    "      </figcaption>\n",
    "    </figure>\n",
    "</div>\n",
    "\n",
    " The output of an NLP model vary based on its objective. Recent advances have enabled the NLP field to evolve from tools like spell check and spam filter to more sophisticated apps such as NLP-powered chatbots, real-time voice-to-text translators, and other sophisticated services.\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img src=\"nlp_img/NLP_application.jpg\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "Along with the achieved groundbreaking results, the size of these models has skyrocketed, reaching millions (or even billions) of parameters, combined with increasing growth in complexity. Therefore, there is a strong need to accelerate neural models inference so that they can meet demanding performance and accuracy requirements in production. Before we try one of the acceleration techniques, let's define the model inference and find out how it differs from the model training. \n",
    "\n",
    "###  Text Model Inference\n",
    "\n",
    "To perform a specific AI task, the model is trained on the known data. During training, the model makes predictions about what the data represents. Any error in the prediction is used to strengthen the artificial neuron connections until the acceptable accuracy level is achieved. The process of training usually done just once and requires massive amount of data and powerful computing systems.\n",
    "\n",
    "The process of using a pre-trained model to make predictions against previously unseen data is called inference. Inference is performed on different devices, happens several times, and consists in feeding the data to the model. In other words, with NLP technologies inferences happen almost all the time, and model output is used by companies and apps for speech recognition, search autocomplete, spam filter, recommendation systems, etc. \n",
    "\n",
    "![](nlp_img/training_inference.png)\n",
    "\n",
    "In this workshop we will work with model inference; in other words, we will execute the model already pre-trained to perform useful work. Let's take a look at the inference on the real-life example of the toxic message detection model.\n",
    "\n",
    "Web services and apps that handle large numbers of user messages might profit from automated moderation systems. On Twitter, tweets are created by thousands of users every minute, and the task of our NLP model is to filter out potentially toxic communications. The model must respond quickly enough so that the structure of the users' conversation is not disrupted.\n",
    "\n",
    "In this case, inference consists in: \n",
    "\n",
    "-\ttaking a text message as input\n",
    "-\ttext preprocessing\n",
    "-\tfeeding that data to our model\n",
    "-\treturning a classification label (toxic/neutral) produced by the model\n",
    "\n",
    "![](nlp_img/inference_nlp_model.png)\n",
    "\n",
    "Besides ensuring fast and safe users' communication, we need to accelerate NLP models to run them in production at a manageable cost. As a result, there is a challenge of accelerating inference of usually complex and large NLP models in the absence of any expensive specialized hardware.\n",
    "\n",
    "So let's find out how the OpenVINO™ toolkit can help in achieving better performance of NLP models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. OpenVINO™ Toolkit <a name=\"OV-overview\"></a>\n",
    "\n",
    "1. [How OpenVINO advances AI technologies](#customers) \n",
    "2. [Introduction to the Openvino toolkit](#OV-intro) \n",
    "3. [OpenVINO capabilities](#OV-capabilities)\n",
    "4. [OpenVINO components](#OV-components)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 How OpenVINO advances AI technologies <a name=\"customers-nlp\"></a>\n",
    "\n",
    "The Open Visual Inference & Neural Network Optimization (OpenVINO™) toolkit is a comprehensive toolkit for optimizing pre-trained Deep Learning models of various use cases to achieve high performance and prepare them for deployment on Intel® platforms. Based on latest generations of artificial neural networks, including convolutional neural networks (CNNs), recurrent and attention-based networks, the toolkit extends computer vision and non-vision workloads across Intel® hardware, maximizing performance. It accelerates applications with high-performance, AI and deep learning inference deployed from edge to cloud.\n",
    "\n",
    "![](nlp_img/sales-collage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Introduction to the OpenVINO™ toolkit <a name=\"OV-intro\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nlp_img/ov_motivation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 OpenVINO™ Capabilities <a name=\"OV-capabilities\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nlp_img/ov_tools.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 OpenVINO™ Toolkit Components <a name=\"OV-components\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nlp_img/additional_tools.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 OpenVINO™ Open Model Zoo <a name=\"OMZ\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nlp_img/omz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deep Learning Workbench: OpenVINO™ Quickstart <a name=\"DL-WB-overview\"></a>\n",
    "\n",
    "1. [DL Workbench Capabilities](#DL-WB-capabilities)\n",
    "2. [DL Workbench Workflow](#DL-WB-workflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning Workbench (DL Workbench) is the official OpenVINO™ graphical interface designed to make the production of pre-trained deep learning models significantly easier.\n",
    "With DL Workbench you can start working with your deep learning model right from your browser: import a model, analyze its performance and accuracy, visualize the outputs, optimize and prepare the model for deployment in a matter of minutes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 DL Workbench Capabilities <a name=\"DL-WB-capabilities\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"nlp_img//DL-WB-flow.png\" width=\"1400\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 DL Workbench Workflow <a name=\"DL-WB-workflow-nlp\"></a>\n",
    "\n",
    "\n",
    "1. [Open DL Workbench](#open-wb-nlp)\n",
    "2. [Import the Model](#import-model-nlp)\n",
    "3. [Import the Dataset](#import-dataset-nlp) \n",
    "4. [Benchmark the Model](#model-inference-nlp)\n",
    "5. [Analyze the Model](#analyze-model-nlp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display button for opening DL Workbench\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"<button class=\"wb-button\" onclick=\"window.open(location.origin, '_blank');\">Open DL Workbench</button><style>.wb-button { display: flex; width: fit-content; margin: 20px auto; align-items: center; height: 50px; font-size: 18px; font-weight: 400; font-family: inherit; line-height: 20px; background-color: #003C71; border: 1px solid #003C71; color: #ffffff; cursor: pointer; border-radius: 4px; padding: 0 30px; box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.15);}</style>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open DL Workbench <a name=\"open-wb-nlp\"></a>\n",
    "\n",
    "To start working with DL Workbench, click **Create Project** button to open the Create Project page.\n",
    "\n",
    "![](nlp_img/start_page_dl_wb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Model <a name=\"import-model-nlp\"></a>\n",
    "\n",
    "The first step on the Create Project page is to **Import the model**. \n",
    "\n",
    "![](nlp_img/import_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select **Original Model** tab, specify **NLP** domain and **ONNX** framework. \n",
    "\n",
    "![](nlp_img/import_nlp_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install requirements and activate ipywidgets for the transformers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download an archive with twint library and unzip it\n",
    "!wget https://github.com/dl-wb-experiments/workshops/files/8035245/twint-2.1.21-py3-none-any.zip\n",
    "!unzip twint-2.1.21-py3-none-any.zip\n",
    "\n",
    "# install requirements\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# enable jupyter extention for transformers library\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download `russian_toxicity_classifier` model from HuggingFace Hub and convert the model to ONNX format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from shutil import rmtree\n",
    "\n",
    "from transformers.convert_graph_to_onnx import convert\n",
    "\n",
    "model_checkpoint = \"chgk13/tiny_russian_toxic_bert\"\n",
    "model_name = model_checkpoint.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = Path(f\"onnx_model/{model_name}.onnx\")\n",
    "if onnx_model_path.parent.exists():\n",
    "    rmtree(onnx_model_path.parent)\n",
    "\n",
    "convert(\n",
    "    framework=\"pt\",\n",
    "    model=model_checkpoint,\n",
    "    output=onnx_model_path,\n",
    "    opset=12,  # check other opsets or try one more time if conversion fails\n",
    "    pipeline_name=\"sentiment-analysis\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model file is placed in the notebook files to the left. Open the `onnx_model` folder: \n",
    "\n",
    "![](nlp_img/model_imported.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right click on the `tiny_russian_toxic_bert` file and select **Download**:\n",
    "\n",
    "![](nlp_img/model_downloaded.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select and upload `.onnx` model file and click **Import**: \n",
    "![](nlp_img/d_import_wizard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with OpenVINO tools, you need to obtain a model in Intermediate Representation (IR) format. \n",
    "IR is the OpenVINO format of pre-trained model representation with two files:\n",
    "\n",
    "- XML file describing the network topology\n",
    "- BIN file containing weights and biases\n",
    "\n",
    "On the third stage **Convert Model to IR - General Parameters**, click **Convert** to proceed:\n",
    "\n",
    "![](nlp_img/d_convert_model.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the fourth step, we need to **configure model inputs**.\n",
    "\n",
    "Layout describes the value of each dimension of input tensors. To configure model layout, set **NC layout**. N is the size of a batch showing how many text samples the model processes at a time. C is the maximum length of text (in tokens) that our model can process.\n",
    "\n",
    "Specify **1 batch** and **128 channels** (tokens) for **each** input. Click **Validate and Import**:\n",
    "\n",
    "![](nlp_img/layout.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model is successfully imported, you will see it on the **Create Project** page. Click on the model to select it and proceed to the **Next Step** :\n",
    "\n",
    "![](nlp_img/d_model_ready.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Environment <a name=\"import-dataset-nlp\"></a>\n",
    "\n",
    "On the **Select Environment** page you can choose a hardware accelerator on which the model will be executed. We will analyze our model on a CPU since we have only this device available. Proceed to the **Next Step**.\n",
    "\n",
    "![](nlp_img/d_select_env.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset <a name=\"import-dataset-nlp\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation of the model is always performed against specific data combined into datasets. You will need to obtain the data to work with the model. The data depends on the task for which the model has been trained.\n",
    "\n",
    "In our case, we take a set of toxic and neutral messages and use them to create our validation dataset.\n",
    "\n",
    "Use the following **link to download the dataset**: [Download Dataset](https://github.com/dl-wb-experiments/workshops/files/8058661/dataset.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the **Select Validation Dataset** page, click **Import Text Dataset**.\n",
    "\n",
    "![](nlp_img/d_text_dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the dataset file. \n",
    "\n",
    "The dataset has a **UTF-8** encoding and **Comma** as separator. In the **Raw Dataset Preview** you can see that our dataset **Has Header**. The dataset will be used for the **Text Classification** task type and contains the text in the **Column 1**, *toxic* (1) and *neutral* (0) labels in the **Column 2** . \n",
    "\n",
    "Make sure the dataset is displayed correctly in the **Formated Dataset Preview** and click **Import**.\n",
    "\n",
    "![](nlp_img/text_dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark the Model <a name=\"model-inference-nlp\"></a>\n",
    "\n",
    "Now that we have imported our model, we will check how fast it works. For that, let's create our first project. \n",
    "\n",
    "Select the model and the dataset by clicking on them, and click **Create Project**.\n",
    "\n",
    "![](nlp_img/create_project_completed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the Model <a name=\"analyze-model-nlp\"></a>\n",
    "\n",
    "When the inference stage is finished, we can see the result of running our model on the CPU. \n",
    "\n",
    "![](nlp_img/d_benchmark_results.png)\n",
    "\n",
    "**Latency** is the time required to process the data. The lower the value, the better.\n",
    "\n",
    " **Throughput** shows how much data the model processes per second. Higher throughput value means better performance. \n",
    "\n",
    "Under the table with results you see a hint saying the model was inferred on the autogenerated data. To infer the model on the text dataset, you need to use a tokenizer. Click **Select Tokenizer** link in the hint and then **Import Tokenizer** button.\n",
    "\n",
    "![](nlp_img/tokenizer_link.png)\n",
    "\n",
    "![](nlp_img/import_tokenizer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Tokenizer <a name=\"import-dataset-nlp\"></a>\n",
    "\n",
    "Tokenizers are used to convert text to numerical data because the model cannot work with the text directly. Tokenizer splits text into tokens. A token can be a word,\n",
    "part of a word, a symbol, or a couple of symbols. Then tokenizer\n",
    "replaces each token with the corresponding index and stores\n",
    "the map between tokens and indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the cells to download a tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "tokenizer.save_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer is placed in the notebook files to the left: \n",
    "\n",
    "![](nlp_img/tokenizer_jupyter.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the tokenizer folder, right click on the *vocab.txt* file and select **Download**:\n",
    "\n",
    "![](nlp_img/download_tokenizer.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To benchmark our model on the text from the imported dataset, we need to:\n",
    "\n",
    "#### Import tokenizer\n",
    "\n",
    "1. Select **WordPiece** as tokenizer type\n",
    "2. Upload `vocab.txt` file\n",
    "3. Click **Import**\n",
    "\n",
    "![](nlp_img/import_tokenizer_1.png)\n",
    "\n",
    "#### Select tokenizer\n",
    "\n",
    "Click on the tokenizer to select it.\n",
    "\n",
    "![](nlp_img/click_on_tokenizer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profile the Model <a name=\"profile-model-nlp\"></a>\n",
    "\n",
    "\n",
    "Open the **Project** tab and go back to the model project:\n",
    "\n",
    "![](nlp_img/d_project.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Select **Perform** and open **Explore Inference Configurations** tab. \n",
    "\n",
    "You can accelerate your model by configuring the optimal inference parameters specific to each accelerator: streams and batches.\n",
    "**Streams** are the number of instances of your model running simultaneously, and **batches** are the number of input data instances fed to the model. To find the optimal configurations and accelerate your model, let's run a range of inference streams. \n",
    "\n",
    "![](nlp_img/batch_stream.svg)\n",
    "\n",
    "Go to the **Perform** tab, open **Explore Inference Configurations** subtab, select Group Inference and click **Configure Group Inference**.\n",
    "\n",
    "![](nlp_img/group_inference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the **Group Inference** page, select combinations of stream and batch parameters by clicking corresponding cells in the table:\n",
    "- Batch 2 and Stream 1\n",
    "- Batch 4 and Stream 4 \n",
    "\n",
    "Click **Execute**:\n",
    "\n",
    "![](nlp_img/group_inference_config.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results. Remember that higher throughput value means better performance. \n",
    "\n",
    "![](nlp_img/benchmark.png)\n",
    "\n",
    "We can see that our model has become 3x times faster with 4 Batch and 4 Stream configuration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s briefly recap what you have learned at this stage:\n",
    "\n",
    "1. What a model is and how it works.\n",
    "2. What model inference is and how to accelerate its speed.\n",
    "3. How to measure and improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Learn OpenVINO™ API <a name=\"OV-API\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Let's learn how to infer a model of text classification use case with OpenVINO™ Python interface and build our application.\n",
    "\n",
    "We will go through the following steps:\n",
    "\n",
    "1. [Import required modules](#Import) \n",
    "2. [Configure inference: path to a model and other data](#Configure)\n",
    "3. [Initialize the OpenVINO™ runtime](#Initialize)\n",
    "4. [Read the model](#Read)\n",
    "5. [Compile the Model for the Device](#Compile)\n",
    "6. [Prepare a text for model inference](#Prepare)\n",
    "7. [Infer the model](#Infer)\n",
    "8. [Show predictions](#Show)\n",
    "9. [Combine inference and result processing](#Combine)\n",
    "10. [Predict labels for a dataset](#Dataset)\n",
    "11. [Make the model dynamic](#Dynamic)\n",
    "\n",
    "\n",
    "**Note**: This OpenVINO API version is currently available only in the opensource repository and will be publicly available in the newest OpenVINO 2022.1 release in March. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Import Required Modules <a name=\"Import\"></a>\n",
    "\n",
    "Import the Python* modules that you will use in the sample code:\n",
    "- [OpenVINO](https://docs.openvinotoolkit.org/latest/openvino_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html) is an OpenVINO™ Python API module used for inference\n",
    "- [NumPy](http://www.numpy.org/) is an array manipulation module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from openvino.runtime import Core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Configure Inference <a name=\"Configure\"></a>\n",
    "\n",
    "Once you have the OpenVINO™ IR of your model, you can start experimenting with it by inferring it and inspecting its output. \n",
    "\n",
    "> **NOTE**: Copy the paths to the `.xml` and `.bin` files from the DL Workbench UI and paste them below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nlp_img/model_path_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements\n",
    "\n",
    "Parameter| Explanation\n",
    "---|---\n",
    "**model_xml**| Path to the `.xml` file of OpenVINO™ IR of your model\n",
    "**model_bin**| Path to the `.bin` file of OpenVINO™ IR of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model IR files\n",
    "model_xml = \n",
    "model_bin = \n",
    "\n",
    "# Path to datset file\n",
    "dataset_path = \n",
    "\n",
    "# Max sequence length for model in tokens\n",
    "sequence_length = 128\n",
    "\n",
    "# Device to use\n",
    "device = \"CPU\"\n",
    "\n",
    "print(\n",
    "    \"Configuration parameters settings:\"\n",
    "    f\"\\n\\tmodel_xml={model_xml}\",\n",
    "    f\"\\n\\tmodel_bin={model_bin}\",\n",
    "    f\"\\n\\tdataset_path={dataset_path}\"\n",
    "    f\"\\n\\tsequence_length={sequence_length}\"\n",
    "    f\"\\n\\tdevice={device}\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Initialize the OpenVINO™ Runtime <a name=\"Initialize\"></a>\n",
    "\n",
    "Once you define the parameters, let's initiate the `Core` object that accesses OpenVINO™ runtime capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Inference Engine instance\n",
    "core = Core()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Read the Model <a name=\"Read\"></a>\n",
    "\n",
    "Put the IR of your model in the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the network from IR files\n",
    "model = core.read_model(model=model_xml, weights=model_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Compile the Model for the Device <a name=\"Compile\"></a>\n",
    "\n",
    "Reading a model is not enough to start a model inference. The model must be loaded to a particular abstraction representing a particular accelerator. A model compiled for a device will be inferred in one of the next steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = core.compile_model(model=model, device_name=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Prepare a Text for Model Inference <a name=\"Prepare\"></a>\n",
    "\n",
    "The model cannot work with the text directly. Instead, the text is first split into tokens and then replaces each token with the corresponding index. A token can be a word, part of a word, a symbol, or a couple of symbols. The map between tokens and indices is stored by a tokenizer.\n",
    "\n",
    "The model input that takes token indices is usually called `input_ids`. There are also might be other inputs. If the model input is static, meaning that it can take only fixed-size inputs, one could have to make an input text longer. For that, there is a special token, called _padding_, that can be added to the beginning or to the end of the sequence. To ignore these tokens during inference there is an `attention_mask` model input.\n",
    "\n",
    "The third part of the tokenized text called `token_type_ids`. If you want to pass a pair of text to the model, token type will separate the first text from the second one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# backup cell for tokenizer reinitialization\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# model_checkpoint = \"chgk13/tiny_russian_toxic_bert\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = [\"Мне фильм понравился\"]\n",
    "\n",
    "tokenized_text = tokenizer(\n",
    "    input_text,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    pad_to_multiple_of=sequence_length,\n",
    "    return_tensors=\"np\",\n",
    ")\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the actual tokens ine could use tokenize method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(input_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Infer the Model <a name=\"Infer\"></a>\n",
    "\n",
    "OpenVINO runs inference via `inference_request` objects. The object contains the input data and the inference result. One can create this object separately, set the input data to it and get the inference result afterward. But we will use the simpler `infer_new_request` method to do all this for us.\n",
    "\n",
    "The `tokenized_text` is a dict-like object. To pass it to the network, we should transform it to a Python dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokenized_text object to dict\n",
    "tokenized_text_dict = {\n",
    "    input_name: input_data for input_name, input_data in tokenized_text.items()\n",
    "}\n",
    "\n",
    "# Save the starting time\n",
    "inf_start = time.time()\n",
    "\n",
    "# Run the inference\n",
    "res = compiled_model.infer_new_request(tokenized_text_dict)\n",
    "\n",
    "# Calculate the time from the start of inference\n",
    "inf_time = time.time() - inf_start\n",
    "print(f\"Inference is complete. Run time: {inf_time * 1000:.3f} ms.\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Show Predictions <a name=\"Show\"></a>\n",
    "\n",
    "Now `res` contains the result of the inference. The model outputs a two-dimension vector that represents two classes: _toxic_ and _neutral_. Some model from HF Hub has a label map from vector dimension index to class label. Let's try to get this mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig\n",
    "\n",
    "\n",
    "# Load index to label map from model config\n",
    "model_config = PretrainedConfig.from_pretrained(model_checkpoint)\n",
    "idx_to_label_dict = getattr(model_config, \"id2label\", {})\n",
    "\n",
    "idx_to_label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can decode a label from output vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that can be applied to vectors\n",
    "# If there is no `idx_to_label` map, the function returns class index\n",
    "idx_to_label = np.vectorize(lambda x: idx_to_label_dict.get(x, x))\n",
    "\n",
    "\n",
    "def process_result(res, compiled_model=compiled_model):\n",
    "    # get model output\n",
    "    res = res[compiled_model.output()]\n",
    "    \n",
    "    # take the position of the maximum element in the vector to get the predicted class index\n",
    "    predicted_class_idx = np.argmax(res, axis=1)\n",
    "    \n",
    "    # transform class index to class label\n",
    "    return idx_to_label(predicted_class_idx)\n",
    "\n",
    "process_result(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nlp_img/toxic_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 Combine Inference and Result Processing <a name=\"Combine\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    tokenized_text = tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        pad_to_multiple_of=sequence_length,\n",
    "        return_tensors=\"np\",\n",
    "    )\n",
    "    res = compiled_model.infer_new_request(dict(**tokenized_text))\n",
    "    logits = res[compiled_model.output()]\n",
    "    predicted_class_idx = np.argmax(logits, axis=1)\n",
    "    return idx_to_label(predicted_class_idx)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predict(\"Шел бы ты отсюда...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.10 Predict Labels for a Dataset <a name=\"Dataset\"></a>\n",
    "\n",
    "Let's get a dataset of different comments and predict the toxicity labels. First, print the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "with open(dataset_path) as csv_file:\n",
    "    dataset_rows = csv.reader(csv_file)\n",
    "    for row in dataset_rows:\n",
    "        print(\" - \".join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the toxicity class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(dataset_path) as csv_file:\n",
    "    dataset_rows = csv.reader(csv_file)\n",
    "    for row in dataset_rows:\n",
    "        prediction = predict(row[0])\n",
    "        row.append(prediction)\n",
    "        print(\" - \".join(row[::-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.11 Make the Model Dynamic <a name=\"Dynamic\"></a>\n",
    "\n",
    "The transformer model has computational complexity $O(n^2)$, where $n$ is the length of the sequence. So, if we increase the length of the sequence with padding, extra calculations will occur and by increasing the length twice, the time might increase by the factor of four. To reduce padding or even eliminate it, you we use a dynamic shape model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "current_model_shape = {\n",
    "    input_.any_name: input_.shape\n",
    "    for input_ in model.inputs\n",
    "}\n",
    "current_model_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openvino.runtime import PartialShape\n",
    "\n",
    "\n",
    "new_model_shape = {\n",
    "    input_.any_name: PartialShape([-1, -1])\n",
    "    for input_ in model.inputs\n",
    "}\n",
    "new_model_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reshape(new_model_shape)\n",
    "dynamic_model = core.compile_model(model=model, device_name=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the starting time\n",
    "inf_start = time.time()\n",
    "\n",
    "# Run the inference\n",
    "res = dynamic_model.infer_new_request(dict(**tokenized_text))\n",
    "\n",
    "# Calculate the time from the start of inference\n",
    "inf_time = time.time() - inf_start\n",
    "print(f\"Inference is complete. Run time: {inf_time * 1000:.3f} ms.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dynamic model is slower than the static on the same input, but now we can reduce the input length and pass the contracted data to the dynamic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = [\"Мне фильм понравился\"]\n",
    "\n",
    "tokenized_text = tokenizer(\n",
    "    input_text,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"np\",\n",
    ")\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the starting time\n",
    "inf_start = time.time()\n",
    "\n",
    "# Run the inference\n",
    "res = dynamic_model.infer_new_request(dict(**tokenized_text))\n",
    "\n",
    "# Calculate the time from the start until now\n",
    "inf_time = time.time() - inf_start\n",
    "print(f\"Inference is complete. Run time: {inf_time * 1000:.3f} ms.\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_result(res, dynamic_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_dynamic(text):\n",
    "    tokenized_text = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"np\",\n",
    "    )\n",
    "    res = dynamic_model.infer_new_request(dict(**tokenized_text))\n",
    "    logits = res[dynamic_model.output()]\n",
    "    predicted_class_idx = np.argmax(logits, axis=1)\n",
    "    return idx_to_label(predicted_class_idx)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dynamic(\"Шел бы ты отсюда...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_path) as csv_file:\n",
    "    dataset_rows = csv.reader(csv_file)\n",
    "    for row in dataset_rows:\n",
    "        prediction = predict_dynamic(row[0])\n",
    "        row.append(prediction)\n",
    "        print(\" - \".join(row[::-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice <a name=\"tasks\"></a>\n",
    "\n",
    "1. [Task 1: ](#task1) Analyze the toxicity of the most popular users on Twitter. \n",
    "2. [Task 2: ](#task2) Check the toxicity of the current Twitter trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from openvino.runtime import Core, PartialShape\n",
    "# from transformers import AutoTokenizer, PretrainedConfig\n",
    "\n",
    "\n",
    "# model_checkpoint = \"chgk13/tiny_russian_toxic_bert\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# model_xml = \n",
    "# model_bin = \n",
    "# device = \"CPU\"\n",
    "\n",
    "# core = Core()\n",
    "# model = core.read_model(model=model_xml, weights=model_bin)\n",
    "# new_model_shape = {\n",
    "#     input_.any_name: PartialShape([-1, -1])\n",
    "#     for input_ in model.inputs\n",
    "# }\n",
    "# model.reshape(new_model_shape)\n",
    "# dynamic_model = core.compile_model(model=model, device_name=device)\n",
    "\n",
    "# model_config = PretrainedConfig.from_pretrained(model_checkpoint)\n",
    "# idx_to_label_dict = getattr(model_config, \"id2label\", {})\n",
    "# idx_to_label = np.vectorize(lambda x: idx_to_label_dict.get(x, x))\n",
    "\n",
    "# def predict_dynamic(text):\n",
    "#     tokenized_text = tokenizer(\n",
    "#         text,\n",
    "#         truncation=True,\n",
    "#         max_length=512,\n",
    "#         return_tensors=\"np\",\n",
    "#     )\n",
    "#     res = dynamic_model.infer_new_request(dict(**tokenized_text))\n",
    "#     logits = res[dynamic_model.output()]\n",
    "#     predicted_class_idx = np.argmax(logits, axis=1)\n",
    "#     return idx_to_label(predicted_class_idx)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 1: Analyze the toxicity of the most popular users on Twitter. <a name=\"task1\"></a>\n",
    "\n",
    "The goal of this task is to scrape a data from Twitter and analyze it with our classifier. We will use a `twint` library to get the data from Twitter. Twint does not require any credentials to parse the data, but it provides limited number of data types from it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import twint libratry and scrape the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import twint\n",
    "import nest_asyncio\n",
    "\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap a twint scraping into a function\n",
    "\n",
    "First we need to create a config for the search. A [list](https://github.com/twintproject/twint/wiki/Configuration) of all configuration options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tweets(\n",
    "    search=None,\n",
    "    username=None,\n",
    "    limit=20,\n",
    "    since=None,  # yyyy-mm-dd\n",
    "    until=None,  # yyyy-mm-dd\n",
    "    filter_retweets=True,\n",
    "    popular_tweets=False,\n",
    "    include_links=True,\n",
    "    **kwargs,\n",
    "):\n",
    "    # clean previously scraped tweets\n",
    "    twint.output.clean_lists()\n",
    "    \n",
    "    # create a config object\n",
    "    config = twint.Config()\n",
    "    \n",
    "    # set config parameters\n",
    "    config.Limit = limit\n",
    "    config.Hide_output = True\n",
    "    config.Username = username\n",
    "    config.Search = search\n",
    "    config.Store_object = True\n",
    "    config.Filter_retweets = filter_retweets\n",
    "    config.Since = since\n",
    "    config.Until = until\n",
    "    config.Popular_tweets = popular_tweets\n",
    "    config.Links = \"include\" if include_links else \"exclude\"\n",
    "    \n",
    "    # set other config attributes that are\n",
    "    for attribute, value in kwargs.items():\n",
    "        setattr(config, attribute, value)\n",
    "\n",
    "    twint.run.Search(config)\n",
    "    \n",
    "    return list(twint.output.tweets_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart the cell a few times if there are no tweets scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tweets for \"opentalksai\"\n",
    "tweets = scrape_tweets(search=\"opentalksai\")\n",
    "\n",
    "# print first 10 tweets\n",
    "for tweet in tweets[:10]:\n",
    "    print()\n",
    "    print(tweet.tweet)\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict tweet class <a name=\"Predict\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function that adds a `.toxicity` attribute to each tweet object in the list. You could use `setattr(x, 'y', v)` or `x.y = v`. To predict a class use `predict_dynamic` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_toxicity_attribute(list_of_tweets):\n",
    "    # loop through list of tweets, predict its toxicity label \n",
    "    # and set an attribute to each one\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a fraction of user's toxic tweets\n",
    "\n",
    "Let's rank a couple of famous Twitter users by the number of toxic tweets. First, we need a function that counts the number of toxic and neutral tweets. You could use `Counter` object from `collections` module. To use counter pass the the a python collection (ex. list or tuple) to it. For example, `Counter(['a', 'a', 'b'])` will result to `{'a': 2, 'b': 1}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def count_twitter_users_toxic_tweets(username, number_of_tweets):\n",
    "    # scrape the most popular tweets of the user\n",
    "    ...\n",
    "    \n",
    "    # apply a toxicity classifier\n",
    "    ...\n",
    "    \n",
    "    # count a number of tweets for each class\n",
    "    tweets_count = ...\n",
    "    \n",
    "    # return counter\n",
    "    return tweets_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the data for some Twitter accounts and count a number of toxic and neutral tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usernames = [\n",
    "    \"VRSoloviev\",\n",
    "    \"Zhirinovskiy\",\n",
    "]\n",
    "number_of_tweets = 40\n",
    "\n",
    "# {username: tweet_counter, ...}\n",
    "results = {}\n",
    "for username in usernames:\n",
    "    # get a tweets count and save it to the result dictionary \n",
    "    tweets_count = ...\n",
    "    \n",
    "    # add the result to the \n",
    "    results[username] = tweets_count\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a Fraction of Toxic Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "for username, counter in results.items():\n",
    "    if counter:\n",
    "        print(username)\n",
    "        plt.pie(counter.values(), labels=counter , autopct='%0.0f%%')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Filter Current Twitter Trends from Toxic Tweets <a name=\"task2\"></a>\n",
    "\n",
    "Let's check the current twitter trends: https://twitter.com/explore/tabs/trending\n",
    "\n",
    "We could use the toxicity classifier to filter out toxic tweets from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# past the choosen trend as a string\n",
    "trend = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, get the tweets for the trend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tweets for a trend\n",
    "trends_tweets = ...\n",
    "\n",
    "# print first 10 tweets\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, filter toxic tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a .toxicity attribute to tweets \n",
    "...\n",
    "\n",
    "# filter the list from toxic tweets\n",
    "filtered_trends_tweets = ...\n",
    "\n",
    "# print 10 tweets from filtered list\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a pipeline to get an automatically curated Twitter feed for a trend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_curated_trends_tweets(trend, limit=100):\n",
    "    # scrape trended tweets, get most popular tweets for a trend\n",
    "    trends_tweets = ...\n",
    "    \n",
    "    # add toxicity predictions \n",
    "    ...\n",
    "    \n",
    "    # filter toxic tweets\n",
    "    filtered_trends_tweets = ...\n",
    "    \n",
    "    # print number of filtered tweets\n",
    "    ...\n",
    "    \n",
    "    return filtered_trends_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a filtered list of tweets and show them in chronological order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get curated tweets list\n",
    "curated_tweets = ...\n",
    "\n",
    "# list the tweets in chronological order\n",
    "# to sort the list you need to provide a fuction \n",
    "\n",
    "sorted_tweets = sorted(\n",
    "    curated_tweets, \n",
    "    # get the `.datetime` attribute from tweet\n",
    "    key=lambda tweet: \n",
    ")\n",
    "\n",
    "# print tweets with spacing\n",
    "for tweet in sorted_list:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"nlp_img/recap_nlp.png\"/>\n",
    "\n",
    "#### Workshop repository: [https://github.com/dl-wb-experiments/workshops](https://github.com/dl-wb-experiments/workshops/tree/master/workshops/2022_opentalksai_nlp)\n",
    "\n",
    "####  Toxicity Classifier model: [https://huggingface.co/chgk13/tiny_russian_toxic_bert](https://huggingface.co/chgk13/tiny_russian_toxic_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workshop partner - **Yandex.Cloud**\n",
    "\n",
    "![](nlp_img/cloud.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**:\n",
    "In this workshop, we worked with a pre-released DL Workbench version which is not publicly available yet. The 2022.1 DL Workbench release will be available in March."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Next steps with OpenVINO <a name=\"next-steps\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Try Workbench right now: locally or in the DevCloud\n",
    "\n",
    "![](nlp_img/call-to-action.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. [Intel® Edge AI Certification](https://www.intel.com/content/www/us/en/developer/tools/devcloud/edge/learn/certification.html)\n",
    "\n",
    "![](nlp_img/edge-ai-certification.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "186.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
