{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</figure>\n",
    "<center> <h1>OpenVINO™ Deep Learning Workbench: NLP TITLE</h1> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. [Introduction](#intro)\n",
    "\n",
    "## 2. [OpenVINO™ Overview](#OV-overview)\n",
    "\n",
    "## 3. [OpenVINO™ Deep Learning Workbench](#DL-WB-overview)\n",
    "\n",
    "## 4. [OpenVINO™ API](#OV-API)\n",
    "\n",
    "## 5. [Practice](#tasks)\n",
    "\n",
    "## 6. [Bonus: Deploy your first OpenVINO application as a telegram bot](#bot)\n",
    "\n",
    "## 7. [Bonus: Next steps with OpenVINO](#next-steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction  <a name=\"intro\"></a>\n",
    "\n",
    "### Workshop Contributors\n",
    "\n",
    "<div style=\"display: block; text-align: center;\">\n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img style=\"border-radius: 50%;\" src=\"pictures/Demidovskij.jpg\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Alexander Demidovskij</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://github.com/demid5111\" style=\"display: inline-flex;\"><img src=\"./pictures/github.svg\" width=\"22px\" style=\"margin-right: 5px\">@demid5111</a>\n",
    "      </figcaption>\n",
    "    </figure>\n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img style=\"border-radius: 50%;\" src=\"pictures/\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Artur Paniukov</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://github.com/apaniukov\" style=\"display: inline-flex;\"><img src=\"./pictures/github.svg\" width=\"22px\" style=\"margin-right: 5px\">@apaniukov</a>\n",
    "      </figcaption>\n",
    "    </figure>\n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img style=\"border-radius: 50%;\" src=\"pictures/Savina.jpg\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Tatiana Savina</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://github.com/tsavina\" style=\"display: inline-flex;\"><img src=\"./pictures/github.svg\" width=\"22px\" style=\"margin-right: 5px\">@tsavina</a>\n",
    "      </figcaption>\n",
    "    </figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### In This Workshop\n",
    "\n",
    "Welcome to the Deep Learning workshop, where you will find out how to start working with pre-trained neural networks and create a transformer-based text classifier applying model optimization techniques. For that, we will be using OpenVINO™ framework and its graphical interface Deep Learning Workbench. \n",
    "\n",
    "During this workshop, you will:\n",
    "\n",
    "1. Learn the basics of neural model analysis and optimization:\n",
    "    - what a model is and how it works\n",
    "    - how model inference works\n",
    "    - how to measure model performance \n",
    "    - how to tune the model for enhanced performance\n",
    "2. Write your own AI application that detects toxic messages.\n",
    "\n",
    "###  NLP and Deep Learning\n",
    "\n",
    "Natural Language Processing (NLP) is a branch of technology that helps programs understand and interpret human language. NLP fills the gap between human communication and machine comprehension. With the rapid development of the NLP field, deep learning models achieve state-of-the-art performance in various business problems such as question answering, sentence comparison, or text summarization. According to IBM's [Global AI Adoption Index 2021](https://newsroom.ibm.com/IBMs-Global-AI-Adoption-Index-2021?lnk=ushpv18ai3), about half of companies are already employing NLP-enabled applications, and more than a quarter plan to do so within the following year. \n",
    "\n",
    "![](nlp_img/graph1.png)\n",
    "![](nlp_img/graph2.png)\n",
    "\n",
    "Typically, NLP algorithms evaluate vast amounts of unstructured text data, such as documents, log files, transcripts, etc. The output of an NLP model might vary based on its objective. Digital Assistants like Alexa (Amazon), Siri (Apple), Cortana (Microsoft), and Google Assistant use NLP to identify speech patterns and infer meaning or complete a task to assist the user. Recent advances have enabled the NLP field to evolve from tools like spell check and spam filter to more sophisticated apps such as NLP-powered chatbots, real-time voice-to-text translators, and other services that can conduct phone calls, track conversations and collect important data from chats and emails, and many more.\n",
    "\n",
    "Along with the achieved groundbreaking results, the size of these models has skyrocketed, reaching millions (or even billions) of parameters, combined with increasing growth in complexity. Therefore, there is a strong need to accelerate neural models inference so that they can meet demanding performance and accuracy requirements in production. Before we try one of the acceleration techniques, let's define the model inference and find out how it differs from the model training. \n",
    "\n",
    "###  Text Model Inference\n",
    "\n",
    "To perform a specific AI task, the model is trained on the known data. During training, the model makes predictions about what the data represents. Any error in the prediction is used to strengthen the artificial neuron connections until the acceptable accuracy level is achieved. The process of training usually done just once and requires massive amount of data and powerful computing systems.\n",
    "\n",
    "The process of using a pre-trained model to make predictions against previously unseen data is called inference. Inference is performed on different devices, happens several times, and consists in feeding the data to the model. In other words, with NLP technologies inferences happen almost all the time, and model output is used by companies and apps for speech recognition, search autocomplete, spam filter, recommendation systems, etc. \n",
    "\n",
    "![](nlp_img/nlp-deep-learning.png)\n",
    "\n",
    "In this workshop we will work with model inference; in other words, we will execute the model already pre-trained to perform useful work. Let's take a look at the inference on the real-life example of the toxic message detection model.\n",
    "\n",
    "Web services and apps that handle large numbers of user messages might profit from automated moderation systems. Potentially, such systems could reduce the cost and time required for manual moderation while increasing the process efficiency. Suppose we have online chats where messages are created by hundreds of users every minute. The messages are processed on the server, and the task of our NLP model is to filter out potentially toxic communications before they reach the chat interface. The model must respond quickly enough so that the structure of the users' conversation is not disrupted.\n",
    "\n",
    "In this case, inference consists in: \n",
    "\n",
    "-\ttaking a text message as input\n",
    "-\ttokenizing it into numerical data\n",
    "-\tfeeding that data to our model\n",
    "-\treturning a classification label (toxic/non-toxic) produced by the model\n",
    "\n",
    "\n",
    "Besides making NLP models fast enough for users, we need to accelerate them and make economical enough to run in production at a manageable cost. As a result, there is a challenge of accelerating inference of usually complex and large NLP models in the absence of any expensive specialized hardware.\n",
    "\n",
    "So let's find out how the OpenVINO™ toolkit can help in achieving better performance of NLP models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. OpenVINO™ Toolkit <a name=\"OV-overview\"></a>\n",
    "\n",
    "1. [How OpenVINO advances AI technologies](#customers) \n",
    "2. [Introduction to the Openvino toolkit](#OV-intro) \n",
    "3. [OpenVINO capabilities](#OV-capabilities)\n",
    "4. [OpenVINO components](#OV-components)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 How OpenVINO advances AI technologies <a name=\"customers\"></a>\n",
    "\n",
    "The Open Visual Inference & Neural Network Optimization (OpenVINO™) toolkit is a comprehensive toolkit for optimizing pre-trained Deep Learning models of various use cases to achieve high performance and prepare them for deployment on Intel® platforms. Based on latest generations of artificial neural networks, including convolutional neural networks (CNNs), recurrent and attention-based networks, the toolkit extends computer vision and non-vision workloads across Intel® hardware, maximizing performance. It accelerates applications with high-performance, AI and deep learning inference deployed from edge to cloud.\n",
    "\n",
    "![](nlp_img/case_studies.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Introduction to the OpenVINO™ toolkit <a name=\"OV-intro\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nlp_img/ov_motivation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/about_vino.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 OpenVINO™ Capabilities <a name=\"OV-capabilities\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nlp_img/ov_tools.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 OpenVINO™ Toolkit Components <a name=\"OV-components\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nlp_img/additional_tools.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 OpenVINO™ Open Model Zoo <a name=\"OMZ\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nlp_img/omz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deep Learning Workbench: OpenVINO™ Quickstart <a name=\"DL-WB-overview\"></a>\n",
    "\n",
    "1. [DL Workbench Capabilities](#DL-WB-capabilities)\n",
    "2. [DL Workbench Workflow](#DL-WB-workflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning Workbench (DL Workbench) is the official OpenVINO™ graphical interface designed to make the production of pre-trained deep learning models significantly easier.\n",
    "With DL Workbench you can start working with your deep learning model right from your browser: import a model, analyze its performance and accuracy, visualize the outputs, optimize and prepare the model for deployment in a matter of minutes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 DL Workbench Capabilities <a name=\"DL-WB-capabilities\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"nlp_img//DL-WB-flow.png\" width=\"1400\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 DL Workbench Workflow <a name=\"DL-WB-workflow\"></a>\n",
    "\n",
    "\n",
    "1. [Open DL Workbench](#open-wb)\n",
    "2. [Import the Model](#import-model)\n",
    "3. [Import the Dataset](#import-dataset) \n",
    "4. [Benchmark the Model](#model-inference)\n",
    "5. [Analyze the Model](#analyze-model) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "1. Download ONNX model\n",
    "\n",
    "2. Download dataset. \n",
    "\n",
    "You will need to obtain the data to work with the model. The data can be in different formats, depending on the task for which the model has been trained. Learn more about these formats in the [documentation](https://docs.openvinotoolkit.org/latest/workbench_docs_Workbench_DG_Dataset_Types.html). \n",
    "\n",
    "In our case, we will take a set of toxic and non-toxic messages and use them as the validation dataset:\n",
    "\n",
    "Use the following **link to download the dataset**: [Download Dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display button for opening DL Workbench\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"<button class=\"wb-button\" onclick=\"window.open(location.origin, '_blank');\">Open DL Workbench</button><style>.wb-button { display: flex; width: fit-content; margin: 20px auto; align-items: center; height: 50px; font-size: 18px; font-weight: 400; font-family: inherit; line-height: 20px; background-color: #003C71; border: 1px solid #003C71; color: #ffffff; cursor: pointer; border-radius: 4px; padding: 0 30px; box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.15);}</style>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open DL Workbench <a name=\"open-wb\"></a>\n",
    "\n",
    "To start working with DL Workbench, click **Create Project** button to open the Create Project page.\n",
    "\n",
    "![](nlp_img/start_page_dl_wb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Model <a name=\"import-model\"></a>\n",
    "\n",
    "Our first step is to **Import model**. \n",
    "\n",
    "![](nlp_img/create_project_model.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select **Original Model** tab, specify **NLP** domain and **ONNX** framework. Select and upload .onnx model file and click **Import**: \n",
    "\n",
    "![](nlp_img/d_import_wizard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with OpenVINO tools, you need to obtain a model in Intermediate Representation (IR) format. \n",
    "IR is the OpenVINO format of pre-trained model representation with two files:\n",
    "\n",
    "- XML file describing the network topology\n",
    "- BIN file containing weights and biases\n",
    "\n",
    "On the third stage **Convert Model to IR - General Parameters**, click **Convert** to proceed:\n",
    "\n",
    "![](nlp_img/d_convert_model.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To configure model layout on the next stage, set **NC layout**, **1 batch** and **128 channels** for each input. Click **Validate and Import**:\n",
    "\n",
    "![](nlp_img/d_layput.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model is successfully imported, you will see it on the page. Click on the model to select it:\n",
    "\n",
    "![](nlp_img/d_model_ready.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset <a name=\"import-dataset\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proceed and click **Next Step**, on the **Select Environment** page you can choose a hardware accelerator on which the model will be executed. We will analyze how our model works on CPU since we have this device available. Go to the **Next Step**.\n",
    "\n",
    "![](nlp_img/d_select_env.png)\n",
    "\n",
    " To **Select Validation Dataset**, click **Import Text Dataset** button.\n",
    "\n",
    "![](nlp_img/d_text_dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the dataset file, make sure it is displayed correctly and click **Import**.\n",
    "![](nlp_img/d_text_preview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark the Model <a name=\"model-inference\"></a>\n",
    "\n",
    "Now that we have imported our model, we want to check how fast it works. For that, let's create our first project. Select the model and the dataset by clicking on them, and click **Create Project**.\n",
    "\n",
    "![](nlp_img/create_project_completed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Select Tokenizer <a name=\"import-dataset\"></a>\n",
    "\n",
    "Tokenizers are used to convert text to numerical data because the model cannot work with the text directly.To work with the model using text from the imported dataset, you need to:\n",
    "\n",
    "- Import tokenizer\n",
    "\n",
    "- Select tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the Model <a name=\"analyze-model\"></a>\n",
    "\n",
    "When the inference stage is finished, we can see the result of running our model on the CPU. Latency is the time required to process the data. The lower the value, the better. Throughput is the number of images (frames) processed per second. Higher throughput value means better performance. Now let's check how the model works and try to make it even faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s briefly recap what you have learned at this stage:\n",
    "\n",
    "1. What a model is and how it works\n",
    "2. What model inference is and why it is important to accelerate its speed\n",
    "3. How to measure model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Learn OpenVINO™ API <a name=\"OV-API\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Let's learn how to infer a model of text classification use case with OpenVINO™ Python interface and build our application. Text classification is a task of predicting the class of the given text..\n",
    "\n",
    "We will go through the following steps:\n",
    "\n",
    "1. [Obtain Required Modules](#1.-Obtain-Required-Modules) \n",
    "2. [_Optional_. Download and convert a pretrained model from the Open Model Zoo](#2.-Optional.-Download-and-Convert-a-Pretrained-Model-from-the-Open-Model-Zoo)\n",
    "3. [Configure inference: path to a model and other data](#3.-Configure-an-Inference)\n",
    "4. [Initialize the OpenVINO™ runtime](#4.-Initialize-the-OpenVINO™-Runtime)\n",
    "5. [Read the model](#5.-Read-the-Model)\n",
    "6. [Make the model executable](#6.-Make-the-Model-Executable)\n",
    "7. [Prepare an image for model inference](#7.-Prepare-an-Image-for-Model-Inference)\n",
    "8. [Infer the model](#8.-Infer-the-Model)\n",
    "9. [Display results](#9.-Display-Results)\n",
    "10. [_Optional_. Compare OpenVINO and PyTorch model](#10.-Optional.-Compare-OpenVINO-and-PyTorch-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Obtain Required Modules\n",
    "Install required modules on your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.19.5 in /home/artur_paniukov/python/fork/workbench/.venv/lib/python3.8/site-packages/numpy-1.19.5-py3.8-linux-x86_64.egg (from -r requirements.txt (line 1)) (1.19.5)\n",
      "Requirement already satisfied: matplotlib==3.3.4 in /home/artur_paniukov/python/fork/workbench/.venv/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (3.3.4)\n",
      "Requirement already satisfied: virtualenv==20.0.30 in /home/artur_paniukov/python/fork/workbench/.venv/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (20.0.30)\n",
      "Requirement already satisfied: torch==1.10.1 in /home/artur_paniukov/python/fork/workbench/.venv/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (1.10.1)\n",
      "Collecting onnx==1.10.2\n",
      "  Using cached onnx-1.10.2-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (12.7 MB)\n",
      "Requirement already satisfied: transformers==4.12.2 in /home/artur_paniukov/python/fork/workbench/.venv/lib/python3.8/site-packages/transformers-4.12.2-py3.8.egg (from -r requirements.txt (line 8)) (4.12.2)\n",
      "Requirement already satisfied: sentencepiece==0.1.96 in /home/artur_paniukov/python/fork/workbench/.venv/lib/python3.8/site-packages/sentencepiece-0.1.96-py3.8-linux-x86_64.egg (from -r requirements.txt (line 9)) (0.1.96)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/artur_paniukov/python/fork/workbench/.venv/lib/python3.8/site-packages/kiwisolver-1.3.2-py3.8-linux-x86_64.egg (from matplotlib==3.3.4->-r requirements.txt (line 2)) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/artur_paniukov/python/fork/workbench/.venv/lib/python3.8/site-packages/python_dateutil-2.8.2-py3.8.egg (from matplotlib==3.3.4->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/artur_paniukov/python/fork/workbench/.venv/lib/python3.8/site-packages/cycler-0.10.0-py3.8.egg (from matplotlib==3.3.4->-r requirements.txt (line 2)) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/artur_paniukov/python/fork/workbench/.venv/lib/python3.8/site-packages/pyparsing-3.0.0rc2-py3.8.egg (from matplotlib==3.3.4->-r requirements.txt (line 2)) (3.0.0rc2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/artur_paniukov/python/fork/workbench/.venv/lib/python3.8/site-packages/Pillow-8.3.2-py3.8-linux-x86_64.egg (from matplotlib==3.3.4->-r requirements.txt (line 2)) (8.3.2)\n",
      "Requirement already satisfied: six<2,>=1.9.0 in /home/artur_paniukov/python/fork/workbench/.venv/lib/python3.8/site-packages (from virtualenv==20.0.30->-r requirements.txt (line 3)) (1.15.0)\n",
      "Requirement already satisfied: appdirs<2,>=1.4.3 in /home/artur_paniukov/python/fork/workbench/.venv/lib/python3.8/site-packages (from virtualenv==20.0.30->-r requirements.txt (line 3)) (1.4.4)\n",
      "Requirement already satisfied: distlib<1,>=0.3.1 in /home/artur_paniukov/python/fork/workbench/.venv/lib/python3.8/site-packages (from virtualenv==20.0.30->-r requirements.txt (line 3)) (0.3.2)\n",
      "Requirement already satisfied: filelock<4,>=3.0.0 in /home/artur_paniukov/python/fork/workbench/.venv/lib/python3.8/site-packages (from virtualenv==20.0.30->-r requirements.txt (line 3)) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions in /home/artur_paniukov/python/fork/workbench/.venv/lib/python3.8/site-packages (from torch==1.10.1->-r requirements.txt (line 5)) (3.7.4.3)\n",
      "Requirement already satisfied: protobuf in /home/artur_paniukov/python/fork/workbench/.venv/lib/python3.8/site-packages (from onnx==1.10.2->-r requirements.txt (line 6)) (3.18.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.17 in /home/artur_paniukov/python/fork/workbench/.venv/lib/python3.8/site-packages/huggingface_hub-0.1.0-py3.8.egg (from transformers==4.12.2->-r requirements.txt (line 8)) (0.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/artur_paniukov/python/fork/workbench/.venv/lib/python3.8/site-packages (from transformers==4.12.2->-r requirements.txt (line 8)) (21.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/artur_paniukov/python/fork/workbench/.venv/lib/python3.8/site-packages (from transformers==4.12.2->-r requirements.txt (line 8)) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/artur_paniukov/python/fork/workbench/.venv/lib/python3.8/site-packages/regex-2021.10.8-py3.8-linux-x86_64.egg (from transformers==4.12.2->-r requirements.txt (line 8)) (2021.10.8)\n",
      "Requirement already satisfied: requests in /home/artur_paniukov/python/fork/workbench/.venv/lib/python3.8/site-packages (from transformers==4.12.2->-r requirements.txt (line 8)) (2.26.0)\n",
      "Requirement already satisfied: sacremoses in /home/artur_paniukov/python/fork/workbench/.venv/lib/python3.8/site-packages/sacremoses-0.0.46-py3.8.egg (from transformers==4.12.2->-r requirements.txt (line 8)) (0.0.46)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/artur_paniukov/python/fork/workbench/.venv/lib/python3.8/site-packages/tokenizers-0.10.3-py3.8-linux-x86_64.egg (from transformers==4.12.2->-r requirements.txt (line 8)) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/artur_paniukov/python/fork/workbench/.venv/lib/python3.8/site-packages/tqdm-4.62.3-py3.8.egg (from transformers==4.12.2->-r requirements.txt (line 8)) (4.62.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/artur_paniukov/python/fork/workbench/.venv/lib/python3.8/site-packages (from requests->transformers==4.12.2->-r requirements.txt (line 8)) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/artur_paniukov/python/fork/workbench/.venv/lib/python3.8/site-packages (from requests->transformers==4.12.2->-r requirements.txt (line 8)) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/artur_paniukov/python/fork/workbench/.venv/lib/python3.8/site-packages (from requests->transformers==4.12.2->-r requirements.txt (line 8)) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/artur_paniukov/python/fork/workbench/.venv/lib/python3.8/site-packages (from requests->transformers==4.12.2->-r requirements.txt (line 8)) (2021.10.8)\n",
      "Requirement already satisfied: click in /home/artur_paniukov/python/fork/workbench/.venv/lib/python3.8/site-packages (from sacremoses->transformers==4.12.2->-r requirements.txt (line 8)) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/artur_paniukov/python/fork/workbench/.venv/lib/python3.8/site-packages/joblib-1.1.0-py3.8.egg (from sacremoses->transformers==4.12.2->-r requirements.txt (line 8)) (1.1.0)\n",
      "Installing collected packages: onnx\n",
      "  Attempting uninstall: onnx\n",
      "    Found existing installation: onnx 1.10.1\n",
      "    Uninstalling onnx-1.10.1:\n",
      "      Successfully uninstalled onnx-1.10.1\n",
      "Successfully installed onnx-1.10.2\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "python3 -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Python* modules that you will use in the sample code:\n",
    "- [os](https://docs.python.org/3/library/os.html#module-os) is a standard Python module used for filename parsing.\n",
    "- [time](https://docs.python.org/3/library/time.html#module-time) is a standard Python module used to measure execution time.\n",
    "- [NumPy](http://www.numpy.org/) is an array manipulation module.\n",
    "- [Deep Learning Inference Engine](https://docs.openvino.ai/latest/openvino_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html) is an OpenVINO™ Python API module used for inference.\n",
    "- [Transformers](https://huggingface.co/docs/transformers/index) if a library for working with NLP models.\n",
    "\n",
    "Run the cell below to import the modules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from shutil import rmtree\n",
    "\n",
    "from openvino.runtime import Core\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, PretrainedConfig\n",
    "from transformers.convert_graph_to_onnx import convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. _Optional_. Download and Convert a Pretrained Model from the HuggingFace Model Hub\n",
    "\n",
    "> **NOTE**: If you already imported a model in the DL Workbench, skip this step and proceed to [configuring inference](#3.-Configure-an-Inference).\n",
    "\n",
    "OpenVINO™ toolkit includes the [Model Optimizer](https://docs.openvino.ai/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html) used to convert and optimize trained models into Intermediate Representation (IR) model files, and the [Inference Engine](https://docs.openvino.ai/latest/openvino_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html), which uses the IR model files to run an inference on hardware devices. The IR model files are created from models trained in popular frameworks, like Caffe\\*, TensorFlow\\*, and others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before downloading a model, you need to configure a Python* environment to convert model from onnx framework. To do this, create a new virtual environment and install required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "wheel_path = Path.cwd().parent.parent / \"wheels\"\n",
    "python_version = f\"{sys.version_info.major}{sys.version_info.minor}\"\n",
    "\n",
    "os.environ[\"OPENVINO_WHEEL\"] = next(str(path.absolute()) for path in wheel_path.iterdir() if python_version in str(path))\n",
    "os.environ[\"OPENVINO_DEV_WHEEL\"] = next(str(path.absolute()) for path in wheel_path.iterdir() if \"dev\" in str(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created virtual environment CPython3.8.10.final.0-64 in 137ms\n",
      "  creator CPython3Posix(dest=/tmp/virtualenvs/onnx, clear=False, global=False)\n",
      "  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/home/artur_paniukov/.local/share/virtualenv)\n",
      "    added seed packages: Pillow==9.0.0, PyRx==0.3.0, PyWavelets==1.2.0, PyYAML==6.0, Shapely==1.8.0, addict==2.4.0, certifi==2021.10.8, charset_normalizer==2.0.10, click==8.0.3, defusedxml==0.7.1, fast_ctc_decode==0.3.0, fastjsonschema==2.15.3, filelock==3.4.2, huggingface_hub==0.4.0, idna==3.3, imagecodecs==2020.5.30, imageio==2.13.5, joblib==1.1.0, jstyleson==0.0.2, lmdb==1.3.0, networkx==2.6.3, nibabel==3.2.1, nltk==3.6.7, numpy==1.19.5, onnx==1.10.2, opencv_python==4.5.5.62, openvino==2022.1.0.dev20220110, openvino_dev==2022.1.0.dev20220110, packaging==21.3, pandas==1.1.5, parasail==1.2.4, pip==20.2.1, pip==21.3.1, progress==1.6, protobuf==3.19.3, py_cpuinfo==8.0.0, pyclipper==1.3.0.post2, pydicom==2.2.2, pyparsing==3.0.6, python_dateutil==2.8.2, pytz==2021.3, rawpy==0.17.0, regex==2021.11.10, requests==2.27.1, sacremoses==0.0.47, scikit_image==0.19.1, scikit_learn==0.24.2, scipy==1.5.4, sentencepiece==0.1.96, setuptools==49.2.1, setuptools==59.4.0, setuptools==59.6.0, six==1.16.0, texttable==1.6.4, threadpoolctl==3.0.0, tifffile==2021.11.2, tokenizers==0.10.3, tqdm==4.62.3, transformers==4.15.0, typing_extensions==4.0.1, urllib3==1.26.8, wheel==0.34.2, wheel==0.36.2\n",
      "  activators BashActivator,CShellActivator,FishActivator,PowerShellActivator,PythonActivator,XonshActivator\n",
      "Requirement already satisfied: pip in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (21.3.1)\n",
      "Processing /home/artur_paniukov/python/fork/workbench/wheels/openvino-2022.1.0.dev20220110-6008-cp38-cp38-manylinux_2_27_x86_64.whl\n",
      "Requirement already satisfied: numpy<1.20,>=1.16.6 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from openvino==2022.1.0.dev20220110) (1.19.5)\n",
      "openvino is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
      "Processing /home/artur_paniukov/python/fork/workbench/wheels/openvino_dev-2022.1.0.dev20220110-6008-py3-none-any.whl\n",
      "Requirement already satisfied: shapely>=1.7.1 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from openvino-dev==2022.1.0.dev20220110) (1.8.0)\n",
      "Requirement already satisfied: fast-ctc-decode>=0.2.5 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from openvino-dev==2022.1.0.dev20220110) (0.3.0)\n",
      "Requirement already satisfied: numpy<1.20,>=1.16.6 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from openvino-dev==2022.1.0.dev20220110) (1.19.5)\n",
      "Requirement already satisfied: pyclipper>=1.2.1 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from openvino-dev==2022.1.0.dev20220110) (1.3.0.post2)\n",
      "Requirement already satisfied: progress>=1.5 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from openvino-dev==2022.1.0.dev20220110) (1.6)\n",
      "Requirement already satisfied: jstyleson~=0.0.2 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from openvino-dev==2022.1.0.dev20220110) (0.0.2)\n",
      "Requirement already satisfied: rawpy>=0.16.0 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from openvino-dev==2022.1.0.dev20220110) (0.17.0)\n",
      "Requirement already satisfied: addict>=2.4.0 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from openvino-dev==2022.1.0.dev20220110) (2.4.0)\n",
      "Requirement already satisfied: pillow>=8.1.2 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from openvino-dev==2022.1.0.dev20220110) (9.0.0)\n",
      "Requirement already satisfied: transformers>=4.5 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from openvino-dev==2022.1.0.dev20220110) (4.15.0)\n",
      "Requirement already satisfied: tokenizers>=0.10.1 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from openvino-dev==2022.1.0.dev20220110) (0.10.3)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from openvino-dev==2022.1.0.dev20220110) (6.0)\n",
      "Requirement already satisfied: nibabel>=3.2.1 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from openvino-dev==2022.1.0.dev20220110) (3.2.1)\n",
      "Requirement already satisfied: parasail>=1.2.4 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from openvino-dev==2022.1.0.dev20220110) (1.2.4)\n",
      "Requirement already satisfied: nltk>=3.5 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from openvino-dev==2022.1.0.dev20220110) (3.6.7)\n",
      "Requirement already satisfied: sentencepiece>=0.1.95 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from openvino-dev==2022.1.0.dev20220110) (0.1.96)\n",
      "Requirement already satisfied: imagecodecs~=2020.5.30 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from openvino-dev==2022.1.0.dev20220110) (2020.5.30)\n",
      "Requirement already satisfied: opencv-python==4.5.* in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from openvino-dev==2022.1.0.dev20220110) (4.5.5.62)\n",
      "Requirement already satisfied: lmdb>=1.2.1 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from openvino-dev==2022.1.0.dev20220110) (1.3.0)\n",
      "Requirement already satisfied: scikit-learn~=0.24.1 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from openvino-dev==2022.1.0.dev20220110) (0.24.2)\n",
      "Requirement already satisfied: requests>=2.25.1 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from openvino-dev==2022.1.0.dev20220110) (2.27.1)\n",
      "Requirement already satisfied: pydicom>=2.1.2 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from openvino-dev==2022.1.0.dev20220110) (2.2.2)\n",
      "Requirement already satisfied: scipy~=1.5.4 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from openvino-dev==2022.1.0.dev20220110) (1.5.4)\n",
      "Requirement already satisfied: pyrx==0.3.0 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from openvino-dev==2022.1.0.dev20220110) (0.3.0)\n",
      "Requirement already satisfied: py-cpuinfo>=7.0.0 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from openvino-dev==2022.1.0.dev20220110) (8.0.0)\n",
      "Requirement already satisfied: pandas~=1.1.5 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from openvino-dev==2022.1.0.dev20220110) (1.1.5)\n",
      "Requirement already satisfied: scikit-image>=0.17.2 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from openvino-dev==2022.1.0.dev20220110) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.54.1 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from openvino-dev==2022.1.0.dev20220110) (4.62.3)\n",
      "Requirement already satisfied: openvino==2022.1.0.dev20220110 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from openvino-dev==2022.1.0.dev20220110) (2022.1.0.dev20220110)\n",
      "Requirement already satisfied: networkx~=2.5 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from openvino-dev==2022.1.0.dev20220110) (2.6.3)\n",
      "Requirement already satisfied: texttable~=1.6.3 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from openvino-dev==2022.1.0.dev20220110) (1.6.4)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from openvino-dev==2022.1.0.dev20220110) (0.7.1)\n",
      "Requirement already satisfied: onnx>=1.8.1 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from openvino-dev==2022.1.0.dev20220110) (1.10.2)\n",
      "Requirement already satisfied: fastjsonschema~=2.15.1 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from openvino-dev==2022.1.0.dev20220110) (2.15.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from nibabel>=3.2.1->openvino-dev==2022.1.0.dev20220110) (21.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from nltk>=3.5->openvino-dev==2022.1.0.dev20220110) (2021.11.10)\n",
      "Requirement already satisfied: click in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from nltk>=3.5->openvino-dev==2022.1.0.dev20220110) (8.0.3)\n",
      "Requirement already satisfied: joblib in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from nltk>=3.5->openvino-dev==2022.1.0.dev20220110) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from onnx>=1.8.1->openvino-dev==2022.1.0.dev20220110) (4.0.1)\n",
      "Requirement already satisfied: protobuf in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from onnx>=1.8.1->openvino-dev==2022.1.0.dev20220110) (3.19.3)\n",
      "Requirement already satisfied: six in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from onnx>=1.8.1->openvino-dev==2022.1.0.dev20220110) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from pandas~=1.1.5->openvino-dev==2022.1.0.dev20220110) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from pandas~=1.1.5->openvino-dev==2022.1.0.dev20220110) (2.8.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from requests>=2.25.1->openvino-dev==2022.1.0.dev20220110) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from requests>=2.25.1->openvino-dev==2022.1.0.dev20220110) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from requests>=2.25.1->openvino-dev==2022.1.0.dev20220110) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from requests>=2.25.1->openvino-dev==2022.1.0.dev20220110) (2.0.10)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from scikit-image>=0.17.2->openvino-dev==2022.1.0.dev20220110) (2.13.5)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from scikit-image>=0.17.2->openvino-dev==2022.1.0.dev20220110) (2021.11.2)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from scikit-image>=0.17.2->openvino-dev==2022.1.0.dev20220110) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from scikit-learn~=0.24.1->openvino-dev==2022.1.0.dev20220110) (3.0.0)\n",
      "Requirement already satisfied: filelock in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from transformers>=4.5->openvino-dev==2022.1.0.dev20220110) (3.4.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from transformers>=4.5->openvino-dev==2022.1.0.dev20220110) (0.4.0)\n",
      "Requirement already satisfied: sacremoses in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from transformers>=4.5->openvino-dev==2022.1.0.dev20220110) (0.0.47)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /tmp/virtualenvs/onnx/lib/python3.8/site-packages (from packaging>=14.3->nibabel>=3.2.1->openvino-dev==2022.1.0.dev20220110) (3.0.6)\n",
      "openvino-dev is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "python3 -m virtualenv /tmp/virtualenvs/onnx\n",
    "source /tmp/virtualenvs/onnx/bin/activate\n",
    "\n",
    "python -m pip install --upgrade pip\n",
    "python -m pip install --upgrade ${OPENVINO_WHEEL}\n",
    "python -m pip install --upgrade ${OPENVINO_DEV_WHEEL}[ONNX]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set a model checkpoint form the HF Model Hub - the [`distilbert-base-uncased-finetuned-sst-2-english`](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"SkolkovoInstitute/russian_toxicity_classifier\"\n",
    "model_name = model_checkpoint.split('/')[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.1 Convert to Pytorch model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX opset version set to: 12\n",
      "Loading pipeline (model: SkolkovoInstitute/russian_toxicity_classifier, tokenizer: SkolkovoInstitute/russian_toxicity_classifier)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51991941a9724f9baf32990e1942c02c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c675f9a8c4214869b0d9b94632d2294b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/712M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bffe5835205044a3812bed943d3a87f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/585 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f98fa236329a48858a0f416c86437e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.40M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045262834e69429aa7aee8f5bc94a3d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating folder onnx_model\n",
      "Using framework PyTorch: 1.7.1+cpu\n",
      "Found input input_ids with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found input token_type_ids with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found input attention_mask with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found output output_0 with shape: {0: 'batch'}\n",
      "Ensuring inputs are in correct order\n",
      "position_ids is not present in the generated input list.\n",
      "Generated inputs order: ['input_ids', 'attention_mask', 'token_type_ids']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/artur_paniukov/python/openvino_notebooks/openvino_notebooks/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:201: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n",
      "/home/artur_paniukov/python/openvino_notebooks/openvino_notebooks/lib/python3.8/site-packages/transformers/modeling_utils.py:2158: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert all(\n"
     ]
    }
   ],
   "source": [
    "onnx_model_path = Path(f\"onnx_model/{model_name}.onnx\")\n",
    "if onnx_model_path.parent.exists():\n",
    "    rmtree(onnx_model_path.parent)\n",
    "\n",
    "convert(\n",
    "    framework=\"pt\", \n",
    "    model=model_checkpoint, \n",
    "    output=onnx_model_path, \n",
    "    opset=12,  # check other opsets or try one more time if conversion fails\n",
    "    pipeline_name=\"sentiment-analysis\",  # default pipeline name for text classification and textual entaimnet models\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Convert ONNX model to OpenVINO IR\n",
    "\n",
    "To convert the model to IR we need to set the shapes of the input tensors, that contains 2 dimentions - _batch size_ and _sequence length_. The names of input tensors are stored in the model `tokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model inputs: input_ids,token_type_ids,attention_mask\n",
      "Input shapes [1,128],[1,128],[1,128]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "batch_size = 1\n",
    "sequence_length = 128\n",
    "\n",
    "assert sequence_length <= tokenizer.model_max_length\n",
    "\n",
    "inputs = \",\".join(tokenizer.model_input_names)\n",
    "input_shapes = \",\".join(f\"[{batch_size},{sequence_length}]\" for _ in range(len(tokenizer.model_input_names)))\n",
    "\n",
    "print(\n",
    "    f\"Model inputs: {inputs}\\n\"\n",
    "    f\"Input shapes {input_shapes}\"\n",
    ")\n",
    "\n",
    "openvino_model_dir = Path(\"ir_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/home/artur_paniukov/python/fork/workbench/tutorials/opentalks/onnx_model/russian_toxicity_classifier.onnx\n",
      "\t- Path for generated IR: \t/home/artur_paniukov/python/fork/workbench/tutorials/opentalks/ir_model\n",
      "\t- IR output name: \trussian_toxicity_classifier\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tinput_ids,token_type_ids,attention_mask\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \t[1,128],[1,128],[1,128]\n",
      "\t- Source layout: \tNot specified\n",
      "\t- Target layout: \tNot specified\n",
      "\t- Layout: \tNot specified\n",
      "\t- Mean values: \tNot specified\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tNone\n",
      "\t- Reverse input channels: \tFalse\n",
      "\t- Use legacy API for model processing: \tFalse\n",
      "\t- Use the transformations config file: \tNone\n",
      "ONNX specific parameters:\n",
      "\t- OpenVINO runtime found in: \t/home/artur_paniukov/intel/openvino_2022/python/python3.8/openvino\n",
      "OpenVINO runtime version: \t2022.1.2022.1.0-6008-8fe5484645a\n",
      "Model Optimizer version: \t2022.1.0-6008-8fe5484645a\n",
      "[ WARNING ] Model Optimizer and OpenVINO runtime versions do no match.\n",
      "[ WARNING ] Consider building the OpenVINO Python API from sources or reinstall OpenVINO (TM) toolkit using \"pip install openvino==2022.1\"\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /home/artur_paniukov/python/fork/workbench/tutorials/opentalks/ir_model/russian_toxicity_classifier.xml\n",
      "[ SUCCESS ] BIN file: /home/artur_paniukov/python/fork/workbench/tutorials/opentalks/ir_model/russian_toxicity_classifier.bin\n",
      "[ SUCCESS ] Total execution time: 38.13 seconds. \n",
      "[ SUCCESS ] Memory consumed: 2166 MB. \n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$onnx_model_path\" \"$inputs\" \"$input_shapes\" \"$openvino_model_dir\"\n",
    "source /tmp/virtualenvs/onnx/bin/activate\n",
    "\n",
    "mo --input_model \"$1\" --input \"$2\" --input_shape \"$3\" --output_dir \"$4\" --data_type \"FP32\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Configure an Inference\n",
    "\n",
    "Once you have the OpenVINO™ IR of your model, you can start experimenting with it by inferring it and inspecting its output. \n",
    "\n",
    "> **NOTE**: If you have the model imported in DL Workbench, copy the paths to the `.xml` and `.bin` files from the DL Workbench UI and paste them below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements\n",
    "\n",
    "Parameter| Explanation\n",
    "---|---\n",
    "**model_xml**| Path to the `.xml` file of OpenVINO™ IR of your model\n",
    "**model_bin**| Path to the `.bin` file of OpenVINO™ IR of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model IR files\n",
    "model_xml = openvino_model_dir / f\"{model_checkpoint}.xml\"\n",
    "model_bin = openvino_model_dir / f\"{model_checkpoint}.bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you define the path to the model, go to the [Initialize the OpenVINO™ Runtime](#Initialize-the-OpenVINO™-Runtime) step and execute all cells one by one to reach the [Show Predictions](#Show-Predictions) step. Then you can return and experiment with optional parameters in the section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional Parameters\n",
    "\n",
    "Experiment with optional parameters after you go the full workflow of the tutorial.\n",
    "\n",
    "Parameter| Explanation\n",
    "---|---\n",
    "**input_image_path**| Path to an input image. Use the `dog.jpg` image placed in the directory of the notebook or, if you have imported a dataset in the DL Workbench, copy the path to an image in the dataset.\n",
    "**device**| Specify the [target device](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Select_Environment.html) to infer on: CPU, GPU, or MYRIAD. Note that the device must be present. For this tutorial, use `CPU` which is known to be present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration parameters settings:\n",
      "\tmodel_xml=ir_model/SkolkovoInstitute/russian_toxicity_classifier.xml \n",
      "\tmodel_bin=ir_model/SkolkovoInstitute/russian_toxicity_classifier.bin \n",
      "\tdevice=CPU\n"
     ]
    }
   ],
   "source": [
    "# Device to use\n",
    "device = \"CPU\"\n",
    "\n",
    "print(\n",
    "    \"Configuration parameters settings:\"\n",
    "    f\"\\n\\tmodel_xml={model_xml}\",\n",
    "    f\"\\n\\tmodel_bin={model_bin}\",\n",
    "    f\"\\n\\tdevice={device}\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Initialize the OpenVINO™ Runtime\n",
    "\n",
    "Once you define the parameters, let's initiate the `Core` object that accesses OpenVINO™ runtime capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Inference Engine instance\n",
    "core = Core()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Read the Model\n",
    "\n",
    "Put the IR of your model in the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Model file ir_model/SkolkovoInstitute/russian_toxicity_classifier.xml cannot be opened!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8132ded57b35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read the network from IR files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_xml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_bin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Set input tensor names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_input_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Model file ir_model/SkolkovoInstitute/russian_toxicity_classifier.xml cannot be opened!"
     ]
    }
   ],
   "source": [
    "# Read the network from IR files\n",
    "model = core.read_model(model=model_xml, weights=model_bin)\n",
    "\n",
    "# Set input tensor names\n",
    "for inp, name in zip(model.inputs, tokenizer.model_input_names):\n",
    "    inp.get_tensor().set_names({name})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Make the Model Executable\n",
    "\n",
    "Reading a network is not enough to start a model inference. The model must be loaded to a particular abstraction representing a particular accelerator. In OpenVINO™, this abstraction is called *plugin*. A network loaded to a plugin becomes executable and will be inferred in one of the next steps.\n",
    "\n",
    "After loading, we keep necessary model information - `output_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = core.compile_model(model=model, device_name=device)\n",
    "\n",
    "output_name = model.output().any_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Prepare a Text for Model Inference\n",
    "\n",
    "The model cannot work with the text directly. Instead, the text is first split into tokens and then replaces each token with the corresponding index. A token can be a word, part of a word, a symbol, or a couple of symbols. The map between tokens and indices is stored by the tokenizer.\n",
    "\n",
    "The model input that takes token indices is usually called `input_ids`. There are also might be other inputs. If the model input is static, meaning that it can take only fixed-size inputs, one could have to make an input text longer. For that, there is a special token, called _padding_, that can be added to the beginning or to the end of the sequence. To ignore these tokens during inference there is an `attention_mask` model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"The movie was good.\"\n",
    "\n",
    "tokenized_text = tokenizer(\n",
    "    input_text,\n",
    "    padding=True,\n",
    "    pad_to_multiple_of=sequence_length,\n",
    "    return_tensors=\"np\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Infer the Model\n",
    "\n",
    "Now that you have the input image in the BGR format and of the right size, you can perform the inference of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the starting time\n",
    "inf_start = time.time()\n",
    "\n",
    "# Run the inference\n",
    "res = compiled_model.infer_new_request(dict(**tokenized_text))\n",
    "res = {output.any_name: output_tensor for output, output_tensor in res.items()}  \n",
    "\n",
    "# Calculate the time from the start until now\n",
    "inf_time = time.time() - inf_start\n",
    "print(f\"Inference is complete. Run time: {inf_time * 1000:.3f} ms.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Display Results\n",
    "\n",
    "Now `res` contains the result of the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to load index to label map from model config\n",
    "model_config = PretrainedConfig.from_pretrained(model_checkpoint)\n",
    "idx_to_label_dict = getattr(model_config, \"id2label\", {})\n",
    "\n",
    "# create function that can be applied to vectors\n",
    "# if there is no `idx_to_label` map, the function returns class index\n",
    "idx_to_label = np.vectorize(lambda x: idx_to_label_dict.get(x, x))\n",
    "\n",
    "def process_result(res):\n",
    "    result_logits = res[output_name]\n",
    "    \n",
    "    # take the position of the maximum element in the vector to get the predicted class index\n",
    "    predicted_class_idx = np.argmax(result_logits, axis=1)\n",
    "    \n",
    "    # transform class index to class label\n",
    "    return idx_to_label(predicted_class_idx)\n",
    "\n",
    "process_result(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. _Optional_. Compare OpenVINO and PyTorch model\n",
    "\n",
    "A model conversion process could fail due to many reasons. For example, the target framework did not support some operations from the source framework. But the model output might change drastically compared to the original model, even if the conversion process is finished without any errors. Since there are two conversions performed in the tutorial (from Pytorch to ONNX and from ONNX to OpenVINO) it is advised to check if the converted model predictions are close to the original model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_logits_is_close(one, other):\n",
    "    is_close = np.isclose(one, other, rtol=1e-04, atol=1e-04)\n",
    "    diff = np.abs(np.abs(one) - np.abs(other))\n",
    "    assert np.all(is_close), f\"Max diff is {np.max(diff * ~is_close)}\"\n",
    "    \n",
    "pytorch_model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "tokenized_text_pt = tokenizer(\n",
    "        input_text,\n",
    "        padding=True,\n",
    "        pad_to_multiple_of=sequence_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "pytorch_res = pytorch_model(**tokenized_text_pt)\n",
    "\n",
    "check_logits_is_close(\n",
    "    res[output_name], \n",
    "    pytorch_res.logits.detach().numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Now you can proceed to importing the model into the DL Workbench or if you have already done that, start exploring numerous features such as:\n",
    "\n",
    "* [Analyse how the model works and its quality](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Visualize_Accuracy.html)\n",
    "* [Perform a baseline inference and analyze model performance](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Run_Single_Inference.html)\n",
    "* [Boost the model by calibrating it to the INT8 precision](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Int_8_Quantization.html)\n",
    "* [Tune the performance of the model by selecting optimal inference parameters](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Run_Range_of_Inferences.html)\n",
    "* [Preparing the model for deployment](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Deploy_and_Integrate_Performance_Criteria_into_Application.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice <a name=\"tasks\"></a>\n",
    "\n",
    "1. [Task 1: ](#task1) \n",
    "2. [Task 2: ](#task2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1:  <a name=\"task1\"></a>\n",
    "\n",
    "The goal of this task is to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2:  <a name=\"task2\"></a>\n",
    "\n",
    "Now that we have learned how to ...., let's try to ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Deploy your first OpenVINO application as a telegram bot <a name=\"bot\"></a>\n",
    "\n",
    "\n",
    "When you find an optimal configuration for your model, the next step is to use this model with optimal parameters in your own application on a target device. OpenVINO™ toolkit includes all you need to run the application on the target. However, the target might have a limited drive space to store all OpenVINO™ components. OpenVINO™ Deployment Manager available inside the DL Workbench extracts the minimum set of libraries required for a target device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Next steps with OpenVINO <a name=\"next-steps\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Try Workbench right now: locally or in DevCloud\n",
    "\n",
    "![](workshops/cv/pictures/call-to-action.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. [Intel® Edge AI Certification](https://www.intel.com/content/www/us/en/developer/tools/devcloud/edge/learn/certification.html)\n",
    "\n",
    "![](pictures/edge-ai-certification.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino_notebooks",
   "language": "python",
   "name": "openvino_notebooks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "186.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
