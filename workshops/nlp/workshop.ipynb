{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</figure>\n",
    "<center> <h1>OpenVINO™ Deep Learning Workbench: NLP TITLE</h1> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. [Introduction](#intro)\n",
    "\n",
    "## 2. [OpenVINO™ Overview](#OV-overview)\n",
    "\n",
    "## 3. [OpenVINO™ Deep Learning Workbench](#DL-WB-overview)\n",
    "\n",
    "## 4. [OpenVINO™ API](#OV-API)\n",
    "\n",
    "## 5. [Practice](#tasks)\n",
    "\n",
    "## 6. [Bonus: Deploy your first OpenVINO application as a telegram bot](#bot)\n",
    "\n",
    "## 7. [Bonus: Next steps with OpenVINO](#next-steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction  <a name=\"intro\"></a>\n",
    "\n",
    "### Workshop Contributors\n",
    "\n",
    "<div style=\"display: block; text-align: center;\">\n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img style=\"border-radius: 50%;\" src=\"pictures/Demidovskij.jpg\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Alexander Demidovskij</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://github.com/demid5111\" style=\"display: inline-flex;\"><img src=\"./pictures/github.svg\" width=\"22px\" style=\"margin-right: 5px\">@demid5111</a>\n",
    "      </figcaption>\n",
    "    </figure>\n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img style=\"border-radius: 50%;\" src=\"pictures/\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Artur Paniukov</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://github.com/apaniukov\" style=\"display: inline-flex;\"><img src=\"./pictures/github.svg\" width=\"22px\" style=\"margin-right: 5px\">@apaniukov</a>\n",
    "      </figcaption>\n",
    "    </figure>\n",
    "    <br>\n",
    "      <figure style=\"display: inline-block;\">\n",
    "      <img style=\"border-radius: 50%;\" src=\"pictures/Tugaryov.jpg\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Artyom Tugaryov</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://github.com/artyomtugaryov\" style=\"display: inline-flex;\"><img src=\"./pictures/github.svg\" width=\"22px\" style=\"margin-right: 5px\">@artyomtugaryov</a>\n",
    "      </figcaption>\n",
    "    </figure>\n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img style=\"border-radius: 50%;\" src=\"pictures/\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Igor Salnikov</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://github.com/SalnikovIgor\" style=\"display: inline-flex;\"><img src=\"./pictures/github.svg\" width=\"22px\" style=\"margin-right: 5px\">@SalnikovIgor</a>\n",
    "      </figcaption>\n",
    "    </figure>\n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img style=\"border-radius: 50%;\" src=\"pictures/Savina.jpg\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Tatiana Savina</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://github.com/tsavina\" style=\"display: inline-flex;\"><img src=\"./pictures/github.svg\" width=\"22px\" style=\"margin-right: 5px\">@tsavina</a>\n",
    "      </figcaption>\n",
    "    </figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### In This Workshop\n",
    "\n",
    "Welcome to the Deep Learning workshop, where you will find out how to start working with pre-trained neural networks and create a transformer-based zero-shot text classifier applying model optimization techniques. For that, we will be using OpenVINO™ framework and its graphical interface Deep Learning Workbench. \n",
    "\n",
    "During this workshop, you will:\n",
    "\n",
    "1. Learn the basics of neural model analysis and optimization:\n",
    "    - what a model is and how it works\n",
    "    - how to measure its performance and analyze the quality\n",
    "    - how to tune the model for enhanced performance\n",
    "2. Write your own AI application that  ...\n",
    "\n",
    "###  NLP and Deep Learning\n",
    "\n",
    "Natural Language Processing is a branch of technology devoted to the nuances of how AI understands human language. NLP technologies are making a significant breakthrough when paired with AI and Deep Learning, achieving state-of-the-art results in practically every NLP-related task.\n",
    "\n",
    "NLP is growing even more popular with ready-to-use pre-trained models and no-code technologies accessible to everyone. Businesses use NLP to improve the operations and make better judgments, in particular:  \n",
    "\n",
    "- better understand their clients and develop more effective tactics\n",
    "- regularly conduct sentiment analysis to gain a better understanding of the business, assess the response of customers to digital marketing effort\n",
    "- extract the most crucial information from a text and summarize it to speed up the process of sifting through massive volumes of data in news articles, legal documents, and scientific papers\n",
    "- aid in automatic translation, copywriting, grammar check and many other tasks. \n",
    "\n",
    "Through Deep Learning we can train models to perform tokenization and experiment with neural models to identify and understand certain rules and patterns that natural languages follow. One of the fundamental tasks in NLP is text classification - the task of assigning a label or class to a given text. Classification models are being successfully applied to solve acute challenging problems such as topic labeling, spam detection, intent detection, and especially sentiment analysis. The neural networks are implemented in sentiment analysis to compute the belongingness of labels. Sentiment analysis is used to mine subjective text, opinions and sentiments to understand feelings, however, one of the big challenges of sentiment analysis is a lack of labelled data. This is where Deep Learning models play a pivotal role, further enriching the applications of NLP.\n",
    "\n",
    "###  NLP Optimization\n",
    "\n",
    "While dealing with enormous amount of text data, model’s performance and accuracy become a challenge. The use of transformer models in production requires significant computational resources, in the DL Workbench you can accelerate neural NLP models, reducing their size and inference time. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. OpenVINO™ Toolkit <a name=\"OV-overview\"></a>\n",
    "\n",
    "1. [SMTH about OV](#customers) \n",
    "2. [Introduction to the Openvino toolkit](#OV-intro) \n",
    "3. [OpenVINO capabilities](#OV-capabilities)\n",
    "4. [OpenVINO components](#OV-components)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 How OpenVINO advances AI technologies <a name=\"customers\"></a>\n",
    "\n",
    "The Open Visual Inference & Neural Network Optimization (OpenVINO™) toolkit is a comprehensive toolkit for optimizing pre-trained Deep Learning models of various use cases to achieve high performance and prepare them for deployment on Intel® platforms. Based on latest generations of artificial neural networks, including convolutional neural networks (CNNs), recurrent and attention-based networks, the toolkit extends computer vision and non-vision workloads across Intel® hardware, maximizing performance. It accelerates applications with high-performance, AI and deep learning inference deployed from edge to cloud.\n",
    "\n",
    "![](nlp_img/case_studies.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Introduction to the OpenVINO™ toolkit <a name=\"OV-intro\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nlp_img/ov_motivation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/about_vino.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 OpenVINO™ Capabilities <a name=\"OV-capabilities\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nlp_img/ov_tools.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 OpenVINO™ Toolkit Components <a name=\"OV-components\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nlp_img/additional_tools.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deep Learning Workbench: OpenVINO™ Quickstart <a name=\"DL-WB-overview\"></a>\n",
    "\n",
    "1. [DL Workbench Capabilities](#DL-WB-capabilities)\n",
    "2. [DL Workbench Workflow](#DL-WB-workflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning Workbench (DL Workbench) is the official OpenVINO™ graphical interface designed to make the production of pre-trained deep learning models significantly easier.\n",
    "With DL Workbench you can start working with your deep learning model right from your browser: import a model, analyze its performance and accuracy, visualize the outputs, optimize and prepare the model for deployment in a matter of minutes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 DL Workbench Capabilities <a name=\"DL-WB-capabilities\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cv/pictures/DL-WB-flow.png\" width=\"1400\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "wheel_path = Path.cwd().parent.parent / \"wheels\"\n",
    "python_version = f\"{sys.version_info.major}{sys.version_info.minor}\"\n",
    "\n",
    "os.environ[\"OPENVINO_WHEEL\"] = next(str(path.absolute()) for path in wheel_path.iterdir() if python_version in str(path))\n",
    "os.environ[\"OPENVINO_DEV_WHEEL\"] = next(str(path.absolute()) for path in wheel_path.iterdir() if \"dev\" in str(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "python3 -m virtualenv /tmp/virtualenvs/onnx\n",
    "source /tmp/virtualenvs/onnx/bin/activate\n",
    "\n",
    "python -m pip install --upgrade pip\n",
    "python -m pip install --upgrade ${OPENVINO_WHEEL}\n",
    "python -m pip install --upgrade ${OPENVINO_DEV_WHEEL}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from shutil import rmtree\n",
    "from transformers.convert_graph_to_onnx import convert\n",
    "\n",
    "\n",
    "model_checkpoint = \"SkolkovoInstitute/russian_toxicity_classifier\"\n",
    "model_name = model_checkpoint.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "onnx_model_path = Path(f\"onnx_model/{model_name}.onnx\")\n",
    "if onnx_model_path.parent.exists():\n",
    "    rmtree(onnx_model_path.parent)\n",
    "\n",
    "convert(\n",
    "    framework=\"pt\",\n",
    "    model=model_checkpoint,\n",
    "    output=onnx_model_path,\n",
    "    opset=12,  # check other opsets or try one more time if conversion fails\n",
    "    pipeline_name=\"sentiment-analysis\",  # default pipeline name for text classification and textual entaimnet models\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 DL Workbench Workflow <a name=\"DL-WB-workflow\"></a>\n",
    "\n",
    "1. [Open DL Workbench](#open-wb)\n",
    "2. [Import the Model](#import-model)\n",
    "3. [Import the Dataset](#import-dataset) \n",
    "4. [Benchmark the Model](#model-inference)\n",
    "5. [Analyze the Model](#analyze-model) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display button for opening DL Workbench\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"<button class=\"wb-button\" onclick=\"window.open(location.origin, '_blank');\">Open DL Workbench</button><style>.wb-button { display: flex; width: fit-content; margin: 20px auto; align-items: center; height: 50px; font-size: 18px; font-weight: 400; font-family: inherit; line-height: 20px; background-color: #003C71; border: 1px solid #003C71; color: #ffffff; cursor: pointer; border-radius: 4px; padding: 0 30px; box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.15);}</style>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open DL Workbench <a name=\"open-wb\"></a>\n",
    "\n",
    "To start working with DL Workbench, click **Create Project** button to open the Create Project page.\n",
    "\n",
    "![](pictures/start_page_dl_wb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Model <a name=\"import-model\"></a>\n",
    "\n",
    "Our first step is to import the model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with OpenVINO tools, you need to obtain a model in Intermediate Representation (IR) format. \n",
    "IR is the OpenVINO format of pre-trained model representation with two files:\n",
    "\n",
    "- XML file describing the network topology\n",
    "- BIN file containing weights and biases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset <a name=\"import-dataset\"></a>\n",
    "\n",
    "Next, you will need to obtain the data to work with the model. The data can be in different formats, depending on the task for which the model has been trained. Learn more about these formats in the [documentation](https://docs.openvinotoolkit.org/latest/workbench_docs_Workbench_DG_Dataset_Types.html). \n",
    "\n",
    "In our case, we will take a set of images and use them as the validation dataset:\n",
    "\n",
    " Use the following **link to download the dataset**: [Download Dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark the Model <a name=\"model-inference\"></a>\n",
    "\n",
    "Now that we have imported our model, we want to check how fast it works. For that, let's create our first project. Select the model and the dataset by clicking on them. You can also choose a hardware accelerator on which the model will be executed. We will analyze how our model works on CPU since we have this device available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the Model <a name=\"analyze-model\"></a>\n",
    "\n",
    "When the inference stage is finished, we can see the result of running our model on the CPU. Latency is the time required to process one image. The lower the value, the better. Throughput is the number of images (frames) processed per second. Higher throughput value means better performance. Now let's check how the model works and try to make it even faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s briefly recap what you have learned at this stage:\n",
    "\n",
    "1. What a model is and how it works\n",
    "2. How to measure its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Learn OpenVINO™ API <a name=\"OV-API\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Let's learn how to infer a model of text classification use case with OpenVINO™ Python interface and build our application. Text classification is a task of predicting the class of the given text..\n",
    "\n",
    "We will go through the following steps:\n",
    "\n",
    "1. [Obtain Required Modules](#1.-Obtain-Required-Modules) \n",
    "2. [_Optional_. Download and convert a pretrained model from the Open Model Zoo](#2.-Optional.-Download-and-Convert-a-Pretrained-Model-from-the-Open-Model-Zoo)\n",
    "3. [Configure inference: path to a model and other data](#3.-Configure-an-Inference)\n",
    "4. [Initialize the OpenVINO™ runtime](#4.-Initialize-the-OpenVINO™-Runtime)\n",
    "5. [Read the model](#5.-Read-the-Model)\n",
    "6. [Make the model executable](#6.-Make-the-Model-Executable)\n",
    "7. [Prepare an image for model inference](#7.-Prepare-an-Image-for-Model-Inference)\n",
    "8. [Infer the model](#8.-Infer-the-Model)\n",
    "9. [Display results](#9.-Display-Results)\n",
    "10. [_Optional_. Compare OpenVINO and PyTorch model](#10.-Optional.-Compare-OpenVINO-and-PyTorch-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from openvino.runtime import Core\n",
    "\n",
    "from transformers import AutoTokenizer, PretrainedConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "batch_size = 1\n",
    "sequence_length = 128\n",
    "\n",
    "assert sequence_length <= tokenizer.model_max_length\n",
    "\n",
    "inputs = \",\".join(tokenizer.model_input_names)\n",
    "input_shapes = \",\".join(f\"[{batch_size},{sequence_length}]\" for _ in range(len(tokenizer.model_input_names)))\n",
    "\n",
    "print(\n",
    "    f\"Model inputs: {inputs}\\n\"\n",
    "    f\"Input shapes {input_shapes}\"\n",
    ")\n",
    "\n",
    "openvino_model_dir = Path(\"ir_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash -s \"$onnx_model_path\" \"$inputs\" \"$input_shapes\" \"$openvino_model_dir\"\n",
    "# source /tmp/virtualenvs/onnx/bin/activate\n",
    "\n",
    "# export PYTHONPATH=/home/artur_paniukov/python/fork/workbench/.unified_venv/lib/python3.8/site-packages:$PYTHONPATH\n",
    "\n",
    "# mo --input_model \"$1\" --input \"$2\" --input_shape \"$3\" --output_dir \"$4\" --data_type \"FP16\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure an Inference\n",
    "\n",
    "Once you have the OpenVINO™ IR of your model, you can start experimenting with it by inferring it and inspecting its output. \n",
    "\n",
    "> **NOTE**: If you have the model imported in DL Workbench, copy the paths to the `.xml` and `.bin` files from the DL Workbench UI and paste them below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements\n",
    "\n",
    "Parameter| Explanation\n",
    "---|---\n",
    "**model_xml**| Path to the `.xml` file of OpenVINO™ IR of your model\n",
    "**model_bin**| Path to the `.bin` file of OpenVINO™ IR of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model IR files\n",
    "model_xml = openvino_model_dir / f\"{model_name}.xml\"\n",
    "model_bin = openvino_model_dir / f\"{model_name}.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device to use\n",
    "device = \"CPU\"\n",
    "\n",
    "print(\n",
    "    \"Configuration parameters settings:\"\n",
    "    f\"\\n\\tmodel_xml={model_xml}\",\n",
    "    f\"\\n\\tmodel_bin={model_bin}\",\n",
    "    f\"\\n\\tdevice={device}\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the OpenVINO™ Runtime\n",
    "\n",
    "Once you define the parameters, let's initiate the `Core` object that accesses OpenVINO™ runtime capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Inference Engine instance\n",
    "core = Core()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Model\n",
    "\n",
    "Put the IR of your model in the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the network from IR files\n",
    "model = core.read_model(model=model_xml, weights=model_bin)\n",
    "\n",
    "# Set input tensor names\n",
    "for inp, name in zip(model.inputs, tokenizer.model_input_names):\n",
    "    inp.get_tensor().set_names({name})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the Model Executable\n",
    "\n",
    "Reading a network is not enough to start a model inference. The model must be loaded to a particular abstraction representing a particular accelerator. In OpenVINO™, this abstraction is called *plugin*. A network loaded to a plugin becomes executable and will be inferred in one of the next steps.\n",
    "\n",
    "After loading, we keep necessary model information - `output_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = core.compile_model(model=model, device_name=device)\n",
    "\n",
    "output_name = model.output().any_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a Text for Inference\n",
    "\n",
    "The model cannot work with the text directly. Instead, the text is first split into tokens and then replaces each token with the corresponding index. A token can be a word, part of a word, a symbol, or a couple of symbols. The map between tokens and indices is stored by the tokenizer.\n",
    "\n",
    "The model input that takes token indices is usually called `input_ids`. There are also might be other inputs. If the model input is static, meaning that it can take only fixed-size inputs, one could have to make an input text longer. For that, there is a special token, called _padding_, that can be added to the beginning or to the end of the sequence. To ignore these tokens during inference there is an `attention_mask` model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = [\"Мне фильм понравился\"]\n",
    "\n",
    "tokenized_text = tokenizer(\n",
    "    input_text,\n",
    "    padding=True,\n",
    "    pad_to_multiple_of=sequence_length,\n",
    "    return_tensors=\"np\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cpu_config = {'PERF_COUNT': 'NO', 'PERFORMANCE_HINT': 'LATENCY', 'CPU_THROUGHPUT_STREAMS': '1'}\n",
    "core.set_config(cpu_config, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the starting time\n",
    "inf_start = time.time()\n",
    "\n",
    "# Run the inference\n",
    "res = compiled_model.infer_new_request(dict(**tokenized_text))[0]\n",
    "\n",
    "# Calculate the time from the start until now\n",
    "inf_time = time.time() - inf_start\n",
    "print(f\"Inference is complete. Run time: {inf_time * 1000:.3f} ms.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "res, res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Results\n",
    "\n",
    "Now `res` contains the result of the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# try to load index to label map from model config\n",
    "model_config = PretrainedConfig.from_pretrained(model_checkpoint)\n",
    "idx_to_label_dict = getattr(model_config, \"id2label\", {})\n",
    "\n",
    "idx_to_label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function that can be applied to vectors\n",
    "# if there is no `idx_to_label` map, the function returns class index\n",
    "idx_to_label = np.vectorize(lambda x: idx_to_label_dict.get(x, x))\n",
    "\n",
    "\n",
    "def process_result(res):\n",
    "    # take the position of the maximum element in the vector to get the predicted class index\n",
    "    predicted_class_idx = ...\n",
    "    predicted_class_idx = np.argmax(res, axis=1)\n",
    "    \n",
    "    # transform class index to class label\n",
    "    return idx_to_label(predicted_class_idx)\n",
    "\n",
    "process_result(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Stream/batch inference\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine inference and result processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    tokenized_text = tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        pad_to_multiple_of=sequence_length,\n",
    "        return_tensors=\"np\",\n",
    "    )\n",
    "    res = compiled_model.infer_new_request(dict(**tokenized_text))[0]\n",
    "    predicted_class_idx = np.argmax(res, axis=1)\n",
    "    return idx_to_label(predicted_class_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predict(\"Шел бы ты отсюда...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Now you can proceed to importing the model into the DL Workbench or if you have already done that, start exploring numerous features such as:\n",
    "\n",
    "* [Analyse how the model works and its quality](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Visualize_Accuracy.html)\n",
    "* [Perform a baseline inference and analyze model performance](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Run_Single_Inference.html)\n",
    "* [Boost the model by calibrating it to the INT8 precision](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Int_8_Quantization.html)\n",
    "* [Tune the performance of the model by selecting optimal inference parameters](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Run_Range_of_Inferences.html)\n",
    "* [Preparing the model for deployment](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Deploy_and_Integrate_Performance_Criteria_into_Application.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice <a name=\"tasks\"></a>\n",
    "\n",
    "1. [Task 1: ](#task1) \n",
    "2. [Task 2: ](#task2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1:  <a name=\"task1\"></a>\n",
    "\n",
    "The goal of this task is to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import twint\n",
    "import nest_asyncio\n",
    "\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = twint.Config()\n",
    "config.Limit = 1\n",
    "config.Username = \"vas3k\"\n",
    "\n",
    "twint.run.Search(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2:  <a name=\"task2\"></a>\n",
    "\n",
    "Now that we have learned how to ...., let's try to ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Deploy your first OpenVINO application as a telegram bot <a name=\"bot\"></a>\n",
    "\n",
    "\n",
    "When you find an optimal configuration for your model, the next step is to use this model with optimal parameters in your own application on a target device. OpenVINO™ toolkit includes all you need to run the application on the target. However, the target might have a limited drive space to store all OpenVINO™ components. OpenVINO™ Deployment Manager available inside the DL Workbench extracts the minimum set of libraries required for a target device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Next steps with OpenVINO <a name=\"next-steps\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Try Workbench right now: locally or in DevCloud\n",
    "\n",
    "![](workshops/cv/pictures/call-to-action.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. [Intel® Edge AI Certification](https://www.intel.com/content/www/us/en/developer/tools/devcloud/edge/learn/certification.html)\n",
    "\n",
    "![](pictures/edge-ai-certification.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino_notebooks",
   "language": "python",
   "name": "openvino_notebooks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "186.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}